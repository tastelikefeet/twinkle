# å¿«é€Ÿå¼€å§‹

## âœ¨ Twinkle æ˜¯ä»€ä¹ˆï¼Ÿ

å¤§æ¨¡å‹è®­ç»ƒç»„ä»¶åº“ã€‚åŸºäº PyTorchï¼Œæ›´ç®€æ´ã€æ›´çµæ´»ã€ç”Ÿäº§å°±ç»ªã€‚

ğŸ§© <b>æ¾è€¦åˆæ¶æ„</b> Â· æ ‡å‡†åŒ–æ¥å£<br>
ğŸš€ <b>å¤šè¿è¡Œæ¨¡å¼</b> Â· torchrun / Ray / HTTP<br>
ğŸ”Œ <b>å¤šæ¡†æ¶å…¼å®¹</b> Â· Transformers / Megatron<br>
ğŸ‘¥ <b>å¤šç§Ÿæˆ·æ”¯æŒ</b> Â· å•åŸºåº§æ¨¡å‹éƒ¨ç½²

## Twinkle é€‚é…æ€§

Twinkle å’Œ [ms-swift](https://github.com/modelscope/ms-swift) éƒ½æ˜¯æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œä½†äºŒè€…çš„ç‰¹æ€§æœ‰å¾ˆå¤§ä¸åŒï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©ã€‚

### ä½•æ—¶é€‰æ‹© Twinkle

- å¦‚æœä½ æ˜¯å¤§æ¨¡å‹çš„åˆå­¦è€…ï¼Œå¸Œæœ›æ›´å¥½åœ°äº†è§£æ¨¡å‹æœºåˆ¶å’Œæ¨¡å‹è®­ç»ƒæ–¹æ³•
- å¦‚æœä½ æ˜¯å¤§æ¨¡å‹ç ”ç©¶è€…ï¼Œå¸Œæœ›å®šåˆ¶æ¨¡å‹æˆ–è®­ç»ƒæ–¹æ³•
- å¦‚æœä½ å–„äºç¼–å†™ training loopï¼Œå¸Œæœ›å®šåˆ¶è®­ç»ƒè¿‡ç¨‹
- å¦‚æœä½ å¸Œæœ›æä¾›ä¼ä¸šçº§æˆ–å•†ä¸šåŒ–è®­ç»ƒå¹³å°

### ä½•æ—¶é€‰æ‹©ms-swift

- å¦‚æœä½ ä¸å…³å¿ƒè®­ç»ƒè¿‡ç¨‹ï¼Œå¸Œæœ›ä»…æä¾›æ•°æ®é›†ä¾¿å¯å®Œæˆè®­ç»ƒ
- å¦‚æœä½ éœ€è¦æ›´å¤šçš„æ¨¡å‹æ”¯æŒå’Œæ•°æ®é›†ç§ç±»
- å¦‚æœä½ éœ€è¦Embeddingã€Rerankerã€Classificationç­‰å¤šç§ç±»å‹çš„è®­ç»ƒ
- å¦‚æœä½ éœ€è¦æ¨ç†ã€éƒ¨ç½²ã€é‡åŒ–ç­‰å…¶ä»–èƒ½åŠ›
- å¦‚æœä½ å¯¹æ–°æ¨¡å‹çš„è®­ç»ƒæ”¯æŒæ•æ„Ÿï¼ŒSwift ä¼šä¿è¯ day-0 çš„æ›´æ–°èƒ½åŠ›

## ä½¿ç”¨æ¨¡å¼

### ä»…ä½¿ç”¨éƒ¨åˆ†ç»„ä»¶

å¼€å‘è€…å¯ä»¥ä»…ä½¿ç”¨Twinkleçš„ä¸€éƒ¨åˆ†ç»„ä»¶ï¼Œç»“åˆè‡ªå·±çš„å·²æœ‰ä»£ç æ¥å®Œæˆè®­ç»ƒå·¥ä½œã€‚ä¾‹å¦‚ï¼Œä»…ä½¿ç”¨Dataset&DataLoaderï¼š

```python
from twinkle.dataset import PackingDataset, DatasetMeta
from twinkle.dataloader import DataLoader
from twinkle.preprocessor import SelfCognitionProcessor

def train():
    dataset_meta = DatasetMeta(
        dataset_id='ms://swift/self-cognition',
    )

    dataset = PackingDataset(dataset_meta)
    dataset.map(SelfCognitionProcessor(model_name='Twinkleæ¨¡å‹', model_author='ModelScopeç¤¾åŒº'))
    dataset.set_template('Template', model_id='ms://Qwen/Qwen3-4B', max_length=512)
    dataset.encode()
    dataset.pack_dataset()

    dataloader = DataLoader(dataset, batch_size=8)
    for data in dataloader:
        print(data)
        """
        {
            "input_ids": [...],
            "position_ids": [...],
            ...
        }
        """
        break

if __name__ == '__main__':
    train()
```
ä¸Šé¢çš„ä»£ç ä¸­ï¼Œä½¿ç”¨PackingDatasetåŠ è½½äº†ä¸€ä¸ªå«åš`swift/self-cognition`çš„æ•°æ®é›†ã€‚PackingDatasetå¯ä»¥ç”¨äºå°†æ•°æ®è¿›è¡Œè£…ç®±ï¼Œä¿è¯æ¯ä¸ªbatchçš„é•¿åº¦éƒ½ä¸è®¾ç½®çš„æœ€å¤§é•¿åº¦ç›¸ä¼¼ã€‚
æˆ‘ä»¬åœ¨å¾ªç¯ä¸­ç®€å•åœ°ä½¿ç”¨äº†printæ‰“å°äº†è¾“å‡ºï¼Œåœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œä½ å¯ä»¥åœ¨ä¸‹é¢ç»§ç»­ç¼–å†™ä½ çš„è‡ªå®šä¹‰è®­ç»ƒä»£ç ã€‚

Twinkleçš„æ‰€æœ‰ç»„ä»¶éƒ½æ”¯æŒå•ç‹¬æ‹†åˆ†ä½¿ç”¨ï¼Œå¯ä»¥å‚è€ƒä¸‹é¢ç« èŠ‚çš„ç»„ä»¶åˆ—è¡¨ã€‚

### å•GPU

Twinkleæ”¯æŒå•GPUè¿è¡Œè®­ç»ƒã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```python
from peft import LoraConfig

from twinkle import get_device_placement, get_logger
from twinkle.dataloader import DataLoader
from twinkle.dataset import Dataset, DatasetMeta
from twinkle.model import TransformersModel
from twinkle.preprocessor import SelfCognitionProcessor

logger = get_logger()


def train():
    # 1000 samples
    dataset = Dataset(dataset_meta=DatasetMeta('ms://swift/self-cognition', data_slice=range(1000)))
    # Set template to prepare encoding
    dataset.set_template('Template', model_id='ms://Qwen/Qwen3-4B')
    # Preprocess the dataset to standard format
    dataset.map(SelfCognitionProcessor('twinkleå¤§æ¨¡å‹', 'ModelScopeç¤¾åŒº'))
    # Encode dataset
    dataset.encode()
    # Global batch size = 8, for GPUs, so 1 sample per GPU
    dataloader = DataLoader(dataset=dataset, batch_size=8)
    # Use a TransformersModel
    model = TransformersModel(model_id='ms://Qwen/Qwen3-4B')

    lora_config = LoraConfig(r=8, lora_alpha=32, target_modules='all-linear')

    # Add a lora to model, with name `default`
    # Comment this to use full-parameter training
    model.add_adapter_to_model('default', lora_config, gradient_accumulation_steps=2)
    # Add Optimizer for lora `default`
    model.set_optimizer(optimizer_cls='AdamW', lr=1e-4)
    # Add LRScheduler for lora `default`
    model.set_lr_scheduler(
        scheduler_cls='CosineWarmupScheduler', num_warmup_steps=5, num_training_steps=len(dataloader))
    logger.info(get_device_placement())
    # Print the training config
    logger.info(model.get_train_configs())
    logger.info(f'Total steps: {len(dataloader)}')
    for step, batch in enumerate(dataloader):
        # Do forward and backward
        model.forward_backward(inputs=batch)
        # Step
        model.clip_grad_and_step()
        if step % 20 == 0:
            # Print metric
            metric = model.calculate_metric(is_training=True)
            logger.info(f'Current is step {step} of {len(dataloader)}, metric: {metric}')
    model.save(f'last-checkpoint')


if __name__ == '__main__':
    train()

```

åœ¨è¿™ä¸ªè®­ç»ƒä»£ç ä¸­ï¼Œæˆ‘ä»¬æ„é€ äº†ä¸€ä¸ªæ•°æ®é›†å¹¶æ‹‰èµ·äº†Qwen/Qwen3-4Bæ¨¡å‹ï¼Œä½¿ç”¨all-linearæ–¹å¼åŠ è½½äº†loraï¼Œå¹¶å®Œæˆäº†ä¸€æ¬¡è®­ç»ƒã€‚åœ¨æ—¥å¿—ä¸­ï¼Œå¯ä»¥çœ‹åˆ°lossé€æ­¥æ”¶æ•›çš„è¿‡ç¨‹ã€‚

### torchrun

Twinkleæ”¯æŒä»¥torchrunæ¨¡å¼è¿è¡Œè®­ç»ƒã€‚åœ¨è¿™ç§åœºæ™¯ä¸‹ï¼Œä¸éœ€è¦å®‰è£…rayç›¸å…³çš„ä¾èµ–ã€‚

```python
from peft import LoraConfig

import twinkle
from twinkle import DeviceMesh, get_device_placement, get_logger
from twinkle.dataloader import DataLoader
from twinkle.dataset import Dataset, DatasetMeta
from twinkle.model import TransformersModel
from twinkle.preprocessor import SelfCognitionProcessor

# Construct a device_mesh, fsdp=4, dp=2
device_mesh = DeviceMesh.from_sizes(fsdp_size=4, dp_size=2)
# use torchrun mode
twinkle.initialize(mode='local', global_device_mesh=device_mesh)

logger = get_logger()


def train():
    # 1000 samples
    dataset = Dataset(dataset_meta=DatasetMeta('ms://swift/self-cognition', data_slice=range(1000)))
    # Set template to prepare encoding
    dataset.set_template('Template', model_id='ms://Qwen/Qwen3-4B')
    # Preprocess the dataset to standard format
    dataset.map(SelfCognitionProcessor('twinkleå¤§æ¨¡å‹', 'ModelScopeç¤¾åŒº'))
    # Encode dataset
    dataset.encode()
    # Global batch size = 8, for GPUs, so 1 sample per GPU
    dataloader = DataLoader(dataset=dataset, batch_size=8)
    # Use a TransformersModel
    model = TransformersModel(model_id='ms://Qwen/Qwen3-4B')

    lora_config = LoraConfig(r=8, lora_alpha=32, target_modules='all-linear')

    # Add a lora to model, with name `default`
    # Comment this to use full-parameter training
    model.add_adapter_to_model('default', lora_config, gradient_accumulation_steps=2)
    # Add Optimizer for lora `default`
    model.set_optimizer(optimizer_cls='AdamW', lr=1e-4)
    # Add LRScheduler for lora `default`
    model.set_lr_scheduler(
        scheduler_cls='CosineWarmupScheduler', num_warmup_steps=5, num_training_steps=len(dataloader))
    logger.info(get_device_placement())
    # Print the training config
    logger.info(model.get_train_configs())
    logger.info(f'Total steps: {len(dataloader)}')
    for step, batch in enumerate(dataloader):
        # Do forward and backward
        model.forward_backward(inputs=batch)
        # Step
        model.clip_grad_and_step()
        if step % 20 == 0:
            # Print metric
            metric = model.calculate_metric(is_training=True)
            logger.info(f'Current is step {step} of {len(dataloader)}, metric: {metric}')
    model.save(f'last-checkpoint')


if __name__ == '__main__':
    train()
```

ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæ„é€ äº†fsdp2å’Œdpçš„hybridå¹¶è¡Œæ¨¡å¼ï¼Œå¹¶ä½¿ç”¨äº†å…«å¼ å¡è¿›è¡Œè®­ç»ƒã€‚å¯ä»¥çœ‹åˆ°å®ƒå’Œå•å¡è®­ç»ƒçš„ä»£ç åŸºæœ¬ç›¸åŒï¼Œåªæ˜¯ä½¿ç”¨äº†`DeviceMesh`æ¥å£°æ˜æ¨¡å‹å¸ƒå±€ã€‚

è¿è¡Œæ—¶ï¼Œéœ€è¦è¿™æ ·æ‹‰èµ·è®­ç»ƒï¼š

```shell
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 torchrun --nproc_per_node=8 train.py
```

### Rayè®­ç»ƒ

[Ray](https://github.com/ray-project/ray)æ˜¯å¤šæœºæ¨¡å‹è®­ç»ƒå’Œæ¨ç†åœºæ™¯ä¸­å¸¸ç”¨çš„è°ƒåº¦ä¸­é—´ä»¶æ¡†æ¶ã€‚å®ƒé’ˆå¯¹å¤šæ¨¡å‹ã€å¤šè®¾å¤‡çš„æ‰§è¡Œå’Œèµ„æºç®¡ç†è¿›è¡Œäº†é¢å¤–ä¼˜åŒ–ï¼Œ
å¹¶æ”¯æŒå¯¹æ¥kubernetesç³»ç»Ÿè¿›è¡Œç”Ÿäº§åŒ–ã€‚è¿™æ ·çš„ç‰¹æ€§ä½¿å¾—å®ƒå°¤å…¶é€‚ç”¨äºRLã€GKDç­‰å¤æ‚è®­ç»ƒåœºæ™¯ä¸­ã€‚

Twinkleæ”¯æŒä½¿ç”¨rayè¿›è¡Œè®­ç»ƒå’Œé‡‡æ ·ï¼Œå¹¶ä¸”å®ƒçš„ä»£ç å’Œä¸Šé¢çš„è®­ç»ƒAPIå‡ ä¹ä¸€è‡´ï¼š

```python
import os
from typing import List, Tuple, Dict, Any
from peft import LoraConfig
import twinkle
from twinkle import DeviceMesh, DeviceGroup, get_device_placement
from twinkle.advantage import GRPOAdvantage
from twinkle.checkpoint_engine import CheckpointEngineManager
from twinkle.data_format import SamplingParams
from twinkle.dataloader import DataLoader
from twinkle.dataset import Dataset, DatasetMeta
from twinkle.model.megatron import MegatronModel
from twinkle.metric import CompletionRewardMetric
from twinkle.preprocessor.llm import GSM8KProcessor
from twinkle.processor import InputProcessor
from twinkle.reward import GSM8KAccuracyReward, GSM8KFormatReward
from twinkle.sampler import vLLMSampler
from twinkle.template import Template

MODEL_ID = os.environ.get('MODEL_ID', 'ms://Qwen/Qwen3-4B')
MODEL_GPUS = int(os.environ.get('MODEL_GPUS', 4))
SAMPLER_GPUS = int(os.environ.get('SAMPLER_GPUS',4))
NUM_GPUS = MODEL_GPUS + SAMPLER_GPUS
NUM_GENERATIONS = int(os.environ.get('NUM_GENERATIONS', 8))
MAX_NEW_TOKENS = int(os.environ.get('MAX_NEW_TOKENS', 4096))
LEARNING_RATE = float(os.environ.get('LR', 1e-5))
MAX_STEPS = int(os.environ.get('MAX_STEPS', 200))
BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 16)) # global prompt-level, global completion-level batch size = BATCH_SIZE * num_generations * dp_size
MINI_BATCH_SIZE = int(os.environ.get('MINI_BATCH_SIZE', 16)) # global completion-level mini-batch-size
MICRO_BATCH_SIZE = int(os.environ.get('MICRO_BATCH_SIZE', 2)) # per-device-micro-batch-size (completion-level), batch_size in forward_backward
GRADIENT_ACCUMULATION_STEPS = int(os.environ.get('GRADIENT_ACCUMULATION_STEPS', 1))
ADAPTER_NAME = 'default'

def create_gsm8k_dataset():
    dataset = Dataset(DatasetMeta('ms://modelscope/gsm8k', subset_name='main', split='train'))
    dataset.set_template('Template', model_id=MODEL_ID, max_length=2048)
    dataset.map(GSM8KProcessor())
    dataset.encode(add_generation_prompt=True)
    return dataset

def compute_rewards(
    trajectories: List[Dict[str, Any]],
) -> Tuple[List[float], List[float], List[float]]:
    accuracy_reward_fn = GSM8KAccuracyReward()
    format_reward_fn = GSM8KFormatReward()
    accuracy_rewards = accuracy_reward_fn(trajectories)
    format_rewards = format_reward_fn(trajectories)
    total_rewards = [a + f for a, f in zip(accuracy_rewards, format_rewards)]
    return total_rewards, format_rewards, accuracy_rewards

def main():
    # set sampler and model separate to use different gpus
    device_groups = [
        DeviceGroup(name='model',ranks=list(range(MODEL_GPUS)),device_type='GPU'),
        DeviceGroup(name='sampler',ranks=list(range(MODEL_GPUS, NUM_GPUS)),device_type='GPU'),
    ]
    model_mesh = DeviceMesh.from_sizes(world_size=MODEL_GPUS, dp_size=MODEL_GPUS)
    sampler_mesh = DeviceMesh.from_sizes(world_size=SAMPLER_GPUS, dp_size=SAMPLER_GPUS)
    twinkle.initialize(mode='ray', nproc_per_node=NUM_GPUS, groups=device_groups, lazy_collect=False)

    lora_config = LoraConfig(target_modules='all-linear', r=32, lora_alpha=64, lora_dropout=0.05)
    model = MegatronModel(model_id=MODEL_ID, device_mesh=model_mesh, remote_group='model', mixed_precision='bf16')
    model.add_adapter_to_model(ADAPTER_NAME, lora_config, gradient_accumulation_steps=1)
    model.set_optimizer('default', lr=LEARNING_RATE)
    model.set_lr_scheduler('default', lr_decay_steps=MAX_STEPS, max_lr=LEARNING_RATE)
    model.set_loss('GRPOLoss', epsilon=0.2)
    model.set_processor(InputProcessor)
    model.set_template('Template', model_id=MODEL_ID)

    sampler = vLLMSampler(
        model_id=MODEL_ID,
        engine_args={
            'gpu_memory_utilization': 0.8,
            'max_model_len': 4096,
            'max_lora_rank': 32, # save as lora_config
            'enable_lora': True,
        },
        device_mesh=sampler_mesh,
        remote_group='sampler',
    )
    sampler.set_template(Template, model_id=MODEL_ID)
    ckpt_manager = CheckpointEngineManager(model=model, sampler=sampler)
    dataloader = DataLoader(
        dataset=create_gsm8k_dataset,
        batch_size=BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,
        min_batch_size=BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,
        device_mesh=model_mesh,
        remote_group='model',
    )
    advantage_fn = GRPOAdvantage()
    metrics = CompletionRewardMetric()
    sampling_params = SamplingParams(max_tokens=MAX_NEW_TOKENS)
    optim_step = 0
    print(get_device_placement())

    for batch in dataloader:
        if optim_step >= MAX_STEPS:
            break
        metrics.reset()
        global_prompts = batch if isinstance(batch, list) else [batch]
        ckpt_manager.sync_weights(merge_and_sync=False)
        sampler.reset_prefix_cache()
        sample_response = sampler.sample(
            global_prompts*NUM_GENERATIONS,
            sampling_params,
            num_samples=1,
        )
        all_input_data: List[Dict[str, Any]] = []
        all_old_logps: List[List[float]] = []
        all_completion_lengths: List[int] = []

        for sequence in sample_response.sequences:
            all_input_data.append(sequence.new_input_feature)
            all_old_logps.append(sequence.logprobs)
            all_completion_lengths.append(len(sequence.tokens))
        total_rewards, format_rewards, accuracy_rewards = compute_rewards(
            all_input_data
        )
        metrics.accumulate(
            completion_lengths=all_completion_lengths,
            rewards={
                'total': total_rewards,
                'format': format_rewards,
                'accuracy': accuracy_rewards,
            },
        )
        advantages = advantage_fn(total_rewards, num_generations=NUM_GENERATIONS, scale='group').tolist()
        # Split completions into mini-batches and run one optim step per mini-batch.
        total_completions = len(all_input_data)
        for mb_start in range(0, total_completions, MINI_BATCH_SIZE):
            mb_end = min(mb_start + MINI_BATCH_SIZE, total_completions)
            mb_inputs = all_input_data[mb_start:mb_end]
            mb_old_logps = all_old_logps[mb_start:mb_end]
            mb_advantages = advantages[mb_start:mb_end]

            model.forward_backward(
                inputs=mb_inputs,
                old_logps=mb_old_logps,
                advantages=mb_advantages,
                micro_batch_size=MICRO_BATCH_SIZE,
            )
            model.clip_grad_and_step()
            optim_step += 1

            if optim_step >= MAX_STEPS:
                break
            log_dict = metrics.calculate()
            log_dict.update(model.calculate_metric(is_training=True))
            metrics.reset()
            print(f'[Step {optim_step}/{MAX_STEPS}] {log_dict}')

    print(f'Training completed. optim_steps={optim_step}')
    model.save('grpo-gsm8k-checkpoint')

if __name__ == '__main__':
    main()
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºäº†ä¸€ä¸ªRLçš„è®­ç»ƒä»£ç ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä»£ç ä¸­æ¸…æ™°çœ‹åˆ°æ•°æ®å¦‚ä½•æ„é€ ã€sampler/modelå¦‚ä½•å£°æ˜å’Œä¼ å‚ï¼Œä»¥åŠadvantageå’Œlossçš„æ„é€ è¿‡ç¨‹ã€‚
è¿™ä¸ªè¿‡ç¨‹æ²¡æœ‰ä»»ä½•æ˜¾ç¤ºå¼•ç”¨`ray`çš„åœ°æ–¹ã€‚æˆ‘ä»¬ä»…åœ¨åˆå§‹åŒ–æ—¶å£°æ˜äº†rayæ¨¡å¼ï¼š

```python
twinkle.initialize(mode='ray', nproc_per_node=NUM_GPUS, groups=device_groups, lazy_collect=False)
```

å¼€å‘è€…å¯ä»¥å®šåˆ¶æ¨¡å‹ç­‰ç»„ä»¶çš„æ„é€ å’Œè°ƒç”¨æ–¹å¼ï¼Œæ‰€æœ‰transformersã€Megatronçš„æ¨¡å‹å‚æ•°éƒ½å¯ä»¥åœ¨æ„é€ æ¨¡å‹æ—¶ä¼ å…¥ã€‚

åé¢æ‰€æœ‰çš„rayè°ƒç”¨å’Œæ•°æ®åˆ†å‘ï¼Œéƒ½æ˜¯éšå¼è¿›è¡Œçš„ã€‚è¿è¡Œè¿™ä¸ªè„šæœ¬éœ€è¦æå‰å®‰è£…å¥½rayã€‚ä¹‹åè¿™æ ·è¿è¡Œï¼š

```shell
python train.py
```

### è¿œç¨‹è®­ç»ƒ

Twinkleçš„ä¸€å¤§ç‰¹è‰²æ˜¯æ”¯æŒå¤šç§Ÿæˆ·ç”¨æˆ·æ··åˆè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œå¤šä¸ªç”¨æˆ·å¯ä»¥ä½¿ç”¨ä¸€ä¸ªåŸºæ¨¡è¿›è¡Œloraè®­ç»ƒï¼Œè¿™æ ·å¯ä»¥æå¤§å‡å°æœåŠ¡ç«¯éƒ¨ç½²æˆæœ¬ã€‚

å‡è®¾æˆ‘ä»¬ä½¿ç”¨å…«å¡å¼€å¯ä¸€ä¸ªæœåŠ¡ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦å¯åŠ¨rayé›†ç¾¤ï¼š

```shell
CUDA_VISIBLE_DEVICES=0,1 ray start --head --port=6379 --num-gpus=2
CUDA_VISIBLE_DEVICES=2,3 ray start --address=127.0.0.1:6379 --num-gpus=2
CUDA_VISIBLE_DEVICES="" ray start --address=127.0.0.1:6379 --num-gpus=0
```

æˆ‘ä»¬å¯åŠ¨äº†ä¸€ç»„åŒ…å«ä¸‰ä¸ªnodeçš„rayé›†ç¾¤ï¼š
- 01ä¸¤å¼ å¡ä½œä¸ºä¸€ä¸ªnode
- 23ä¸¤å¼ å¡ä½œä¸ºä¸€ä¸ªnode
- cpuèµ„æºä½œä¸ºä¸€ä¸ªnode

å¦‚æœåœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Œå¯ä»¥å¯åŠ¨æ›´å¤šnodeï¼Œå¹¶éƒ¨ç½²æ›´å¤šreplicaä»¥å…¼å®¹æ›´å¤§çš„ç”¨æˆ·é‡ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä»…ä»¥å››å¡ä½œä¸ºä¾‹å­ã€‚

ä¸‹é¢ï¼Œå¯åŠ¨serverï¼š
```shell

cd cookbook/client/twinkle/transformer
python server.py
```

æœåŠ¡ç«¯ä¼šå¯åŠ¨ä¸€ä¸ªåŒ…å«äº†ä¸€ä¸ªsampleré›†ç¾¤ã€ä¸€ä¸ªæ¨¡å‹é›†ç¾¤ã€ä¸€ä¸ªå·¥å…·é›†ç¾¤çš„ä¸‰ä¸ªæœåŠ¡ã€‚

ä¸‹é¢å¯ä»¥è¿›è¡Œclientç«¯è®­ç»ƒï¼š
```python
import dotenv
dotenv.load_dotenv('.env')
import re
from twinkle.data_format import Trajectory
from twinkle.reward.base import Reward
import gc
from peft import LoraConfig
from typing import List, Tuple

from twinkle import get_logger
from twinkle.advantage import GRPOAdvantage
from twinkle.dataset import DatasetMeta
from twinkle.metric import CompletionRewardMetric
from twinkle_client import init_twinkle_client
from twinkle_client.dataloader import DataLoader
from twinkle_client.dataset import Dataset
from twinkle_client.model import MultiLoraTransformersModel
from twinkle_client.sampler import vLLMSampler

logger = get_logger()

# ========== Configuration ==========
MODEL_ID = 'ms://Qwen/Qwen3-4B'
NUM_GENERATIONS = 4
MAX_NEW_TOKENS = 1024
LEARNING_RATE = 1e-5
MAX_STEPS = 10
BATCH_SIZE = 2
TEMPERATURE = 1.0
SYNC_INTERVAL = 1  # Save weights for sampler every N steps
GRADIENT_ACCUMULATION_STEPS = 4


def create_countdown_dataset():
    """Create Countdown Game dataset for GRPO training."""

    dataset = Dataset(dataset_meta=DatasetMeta('ms://zouxuhong/Countdown-Tasks-3to4', data_slice=range(500)))
    dataset.set_template('Template', model_id=MODEL_ID, max_length=8192)
    dataset.map('CountdownProcessor')
    dataset.encode(add_generation_prompt=True, batched=True)
    return dataset


class CountDownAccuracy(Reward):

    @staticmethod
    def countdown_accuracy_reward(completion: str, target: int, nums: List[int]) -> float:
        """Accuracy reward: checks if equation is correct."""
        try:
            match = re.search(r'<answer>(.*?)<\/answer>', completion)
            if match is None:
                return 0.0
            equation = match.group(1).strip()
            if '=' in equation:
                equation = equation.split('=')[0]
            used_numbers = [int(n) for n in re.findall(r'\d+', equation)]
            if sorted(used_numbers) != sorted(nums):
                return 0.0
            if not re.match(r'^[\d+\-*/().\s]+$', equation):
                return 0.0
            result = eval(equation, {'__builtins__': None}, {})
            return 1.0 if abs(float(result) - float(target)) < 1e-5 else 0.0
        except Exception:  # noqa
            return 0.0

    def __call__(self, trajectories: List[Trajectory], ground_truths: List[Trajectory]):
        rewards = []
        for trajectory in trajectories:
            messages = trajectory.get('messages', [])
            completion = ''
            for msg in reversed(messages):
                if msg.get('role') == 'assistant':
                    completion = msg.get('content', '')
                    break
            user_data = trajectory.get('user_data', [{}])
            data = user_data[0] if isinstance(user_data, list) and user_data else {}
            target = data.get('target', 0)
            nums = data.get('nums', [])
            acc_reward = self.countdown_accuracy_reward(completion, target, nums)
            rewards.append(acc_reward)
        return rewards


def compute_rewards(trajectories: List[dict], ) -> Tuple[List[float], List[float], List[float]]:
    """Compute format and accuracy rewards for Countdown game."""
    from twinkle.reward import FormatReward
    format_rewards = FormatReward()(trajectories, [])
    accuracy_rewards = CountDownAccuracy()(trajectories, [])
    total_rewards = [a + b for a, b in zip(accuracy_rewards, format_rewards)]
    return total_rewards, format_rewards, accuracy_rewards


def train():
    # Step 1: Initialize the Twinkle client
    client = init_twinkle_client(
        base_url='http://localhost:8000',
        api_key='',
    )

    # Step 2: Prepare dataset and dataloader
    dataset = create_countdown_dataset()
    dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE)

    # Step 3: Configure the training model
    model = MultiLoraTransformersModel(model_id=MODEL_ID)

    lora_config = LoraConfig(
        target_modules='all-linear',
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
    )
    model.add_adapter_to_model(
        'default',
        lora_config,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    )

    # Set GRPO loss (the key difference from SFT training)
    model.set_loss('GRPOLoss', epsilon=0.2, beta=0.0)

    # Set optimizer and LR scheduler
    model.set_optimizer('AdamW', lr=LEARNING_RATE)
    model.set_lr_scheduler(
        'CosineWarmupScheduler',
        num_warmup_steps=500,
        num_training_steps=MAX_STEPS,
    )

    # Set processor and template for encoding inputs
    model.set_processor('InputProcessor')
    model.set_template('Template', model_id=MODEL_ID)

    # Step 4: Configure the sampler
    sampler = vLLMSampler(model_id=MODEL_ID)
    sampler.set_template('Template', model_id=MODEL_ID)

    # Step 5: Setup metrics and advantage function
    advantage_fn = GRPOAdvantage()
    metrics = CompletionRewardMetric()

    sampling_params = {
        'max_tokens': MAX_NEW_TOKENS,
        'temperature': TEMPERATURE,
        'top_p': 0.95,
    }

    # Track the current adapter path for sampling
    current_adapter_uri = None

    step = 0
    for batch in dataloader:
        if step >= MAX_STEPS:
            break

        metrics.reset()
        prompts = batch if isinstance(batch, list) else [batch]

        # ========== 1. Save weights and update adapter_uri ==========
        # Instead of sync_weights, save the model checkpoint and pass
        # the resulting path to the sampler as adapter_uri
        if step % SYNC_INTERVAL == 0:
            logger.info(f'Step {step}: Saving weights for sampler...')
            twinkle_path = model.save(
                name=f'grpo-sampler-step-{step}',
                save_optimizer=False,
            )
            current_adapter_uri = twinkle_path
            logger.info(f'Step {step}: Saved weights to {current_adapter_uri}')

        # ========== 2. Sample completions ==========
        sample_response = sampler.sample(
            inputs=prompts,
            sampling_params=sampling_params,
            adapter_uri=current_adapter_uri,
            num_samples=NUM_GENERATIONS,
        )

        input_features = []
        old_logps_list = []
        completion_lengths = []

        sequences = sample_response.get('sequences', [])
        for seq in sequences:
            input_features.append(seq.get('new_input_feature', seq))
            old_logps_list.append(seq.get('logprobs', []))
            completion_lengths.append(len(seq.get('tokens', [])))

        if not input_features:
            logger.warning(f'Step {step}: No valid samples, skipping')
            step += 1
            continue

        # ========== 3. Compute rewards ==========
        total_rewards, format_rewards, accuracy_rewards = compute_rewards(input_features)
        metrics.accumulate(
            None,
            None,
            completion_lengths=completion_lengths,
            rewards={
                'total': total_rewards,
                'format': format_rewards,
                'accuracy': accuracy_rewards,
            })

        # ========== 4. Compute advantages ==========
        advantages = advantage_fn(
            total_rewards,
            num_generations=NUM_GENERATIONS,
            scale='group',
        ).tolist()

        frac_zero_std = (1.0 if all(abs(a) < 1e-8 for a in advantages) else 0.0)
        if frac_zero_std == 1.0:
            logger.info(f'Step {step}: All advantages are zero, skipping training')
            step += 1
            continue

        # ========== 5. Training step (GRPO) ==========
        # forward_backward with GRPO loss: passes advantages and old_logps
        # to the server-side GRPOLoss for proper policy optimization
        model.forward_backward(
            inputs=input_features,
            advantages=advantages,
            old_logps=old_logps_list,
        )

        # Gradient clipping and optimizer step
        model.clip_grad_norm(1.0)
        model.step()
        model.zero_grad()
        model.lr_step()

        gc.collect()

        # ========== 6. Log ==========
        log_dict = metrics.calculate()
        log_dict.update(model.calculate_metric())
        log_dict['train/frac_reward_zero_std'] = frac_zero_std
        logger.info(f'Step {step}: {log_dict}')
        step += 1

    # Save final checkpoint
    twinkle_path = model.save(name='grpo-countdown-final', save_optimizer=True)
    logger.info(f'Saved final checkpoint: {twinkle_path}')


if __name__ == '__main__':
    train()
```

å¤šä¸ªå¼€å‘è€…å¯ä»¥å¹¶è¡Œä½¿ç”¨è¿™ä¸ªæœåŠ¡çš„å•ä¸ªåŸºæ¨¡å¹¶è¡Œè®­ç»ƒå’Œé‡‡æ ·ã€‚å¹¶ä¸”ï¼Œä»–ä»¬è¿›è¡Œçš„è®­ç»ƒæ–¹å¼å…è®¸ä¸åŒã€‚ä¾‹å¦‚ï¼ŒAç”¨æˆ·å¯ä»¥è¿›è¡ŒSFTï¼ŒBç”¨æˆ·å¯ä»¥è¿›è¡ŒRLï¼ŒCç”¨æˆ·å¯ä»¥è¿›è¡Œé‡‡æ ·ã€‚ åŒæ ·ï¼ŒTwinkleä¹Ÿæ”¯æŒTinker-like APIè¿›è¡Œè¿œç«¯è®­ç»ƒï¼š

>[!Note]
> éœ€è¦æ³¨æ„çš„ä¸€ç‚¹ï¼Œåœ¨å½“å‰Twinkleçš„å®ç°ä¸­ï¼Œå®¢æˆ·ç«¯çš„Twinkle APIå’ŒTinker APIæ˜¯æ— æ³•åŒæ—¶åœ¨ä¸€ä¸ªæœåŠ¡ç«¯ä½¿ç”¨çš„ã€‚å½“ä½ éœ€è¦æä¾›Tinker APIæ—¶ï¼Œä½ éœ€è¦å¯åŠ¨cookbook/client/tinkerä¸‹çš„æœåŠ¡ã€‚
> è¿™ä¸ªé—®é¢˜ä¼šåœ¨æ¥ä¸‹æ¥çš„è¿­ä»£é«˜ä¼˜è§£å†³ã€‚

```python
from tinker import types
from tqdm import tqdm
from tinker import ServiceClient
from twinkle.dataloader import DataLoader
from twinkle.dataset import Dataset, DatasetMeta
from twinkle.preprocessor import SelfCognitionProcessor
from twinkle.server.tinker.common import input_feature_to_datum

# The base model to fine-tune / evaluate
base_model = 'Qwen/Qwen3-4B'


def train():
    # Step 1: Prepare the dataset

    # Load the self-cognition dataset from ModelScope (first 500 examples)
    dataset = Dataset(dataset_meta=DatasetMeta('ms://swift/self-cognition', data_slice=range(500)))

    # Apply the chat template matching the base model (max 256 tokens per sample)
    dataset.set_template('Template', model_id=f'ms://{base_model}', max_length=256)

    # Replace placeholder names with custom model/author identity
    dataset.map(SelfCognitionProcessor('twinkleæ¨¡å‹', 'twinkleå›¢é˜Ÿ'), load_from_cache_file=False)

    # Tokenize and encode the dataset into model-ready input features
    dataset.encode(batched=True, load_from_cache_file=False)

    # Wrap the dataset into a DataLoader that yields batches of size 8
    dataloader = DataLoader(dataset=dataset, batch_size=8)

    # Step 2: Initialize the training client
    # Connect to the Twinkle server running locally
    service_client = ServiceClient(base_url='http://localhost:8000', api_key='your-api-key')
    # Create a LoRA training client for the base model (rank=16 for the LoRA adapter)
    training_client = service_client.create_lora_training_client(base_model=base_model, rank=16)

    # Step 3: Run the training loop
    for epoch in range(3):
        print(f'Epoch {epoch}')
        for step, batch in tqdm(enumerate(dataloader)):
            # Convert each InputFeature into a Datum for the Tinker API
            input_datum = [input_feature_to_datum(input_feature) for input_feature in batch]

            # Send data to server: forward + backward pass (computes gradients)
            fwdbwd_future = training_client.forward_backward(input_datum, 'cross_entropy')

            # Optimizer step: update model weights with Adam
            optim_future = training_client.optim_step(types.AdamParams(learning_rate=1e-4))

            # Wait for both operations to complete
            fwdbwd_future.result()
            optim_result = optim_future.result()
            print(f'Training Metrics: {optim_result}')

        # Save a checkpoint after each epoch
        save_future = training_client.save_state(f'twinkle-lora-{epoch}')
        save_result = save_future.result()
        print(f'Saved checkpoint to {save_result.path}')


if __name__ == '__main__':
    train()
```

### ä½¿ç”¨é­”æ­ç¤¾åŒºæä¾›çš„TaaSåŒ–è®­ç»ƒæœåŠ¡

åœ¨ Twinkle æ¡†æ¶å¼€æºçš„åŒæ—¶ï¼Œæˆ‘ä»¬ä¾æ‰˜ModelScopeçš„åå°æœåŠ¡ï¼Œä¹Ÿæä¾›äº†æ‰˜ç®¡çš„æ¨¡å‹è®­ç»ƒæœåŠ¡(Training as a Serviceï¼‰ï¼Œå¼€å‘è€…å¯ä»¥é€šè¿‡è¿™ä¸€æœåŠ¡ï¼Œ å…è´¹ä½“éªŒTwinkleçš„è®­ç»ƒAPIã€‚
è¯¥æœåŠ¡å’Œä¸Šé¢å™è¿°çš„Tinker APIéƒ¨åˆ†ä»£ç æ˜¯ç›¸åŒçš„ï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯Endpointå’ŒTokenéœ€è¦ä½¿ç”¨é­”æ­å®˜æ–¹çš„å¯¹åº”ä¿¡æ¯ã€‚å…³äºå¦‚ä½•ä½¿ç”¨å®˜æ–¹æœåŠ¡ï¼Œè¯·æŸ¥çœ‹[è®­ç»ƒæœåŠ¡](./è®­ç»ƒæœåŠ¡.md)çš„è¯¦ç»†æè¿°ã€‚

## ğŸ› ï¸ Twinkleâœ¨ æ¨¡å—åŒ–ç”Ÿæ€ç³»ç»Ÿ

<div align="center">
  <table style="width: 100%; border-collapse: separate; border-spacing: 8px;">
    <tr>
      <td width="20%" bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Dataset</b><br><sub>æ•°æ®åŠ è½½å’Œé¢„å¤„ç†</sub></p>
      </td>
      <td width="20%" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Template</b><br><sub>ç¼–ç å’Œè§£ç </sub></p>
      </td>
      <td width="20%" bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>DataLoader</b><br><sub>æ•°æ®åˆ†å‘å’Œæ‰¹å¤„ç†</sub></p>
      </td>
      <td width="20%" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Preprocessor</b><br><sub>æ•°æ® ETL</sub></p>
      </td>
      <td width="20%" bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>InputProcessor</b><br><sub>ä»»åŠ¡ç‰¹å®šçš„è¾“å…¥å¤„ç†</sub></p>
      </td>
    </tr>
    <tr>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Model</b><br><sub>å¤§æ¨¡å‹ï¼Œæ”¯æŒå¤šç§æ¡†æ¶</sub></p>
      </td>
      <td style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Sampler</b><br><sub>é‡‡æ ·é€»è¾‘</sub></p>
      </td>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Loss</b><br><sub>æŸå¤±å‡½æ•°</sub></p>
      </td>
      <td style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Metric</b><br><sub>è®­ç»ƒæŒ‡æ ‡æ”¶é›†</sub></p>
      </td>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Reward</b><br><sub>å¥–åŠ±å‡½æ•°</sub></p>
      </td>
    </tr>
    <tr>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Advantage</b><br><sub>ä¼˜åŠ¿å‡½æ•°</sub></p>
      </td>
      <td style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>CheckpointEngine</b><br><sub>æƒé‡åŒæ­¥</sub></p>
      </td>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Patch</b><br><sub>æ¨¡å‹ä¿®å¤è¡¥ä¸</sub></p>
      </td>
      <td style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Module</b><br><sub>ç»„ä»¶ï¼Œå¦‚ä¼˜åŒ–å™¨</sub></p>
      </td>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Kernel</b><br><sub>ç®—å­</sub></p>
      </td>
    </tr>
    <tr>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Server</b><br><sub>å¯åŠ¨åç«¯é›†ç¾¤</sub></p>
      </td>
      <td style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Client</b><br><sub>å®¢æˆ·ç«¯ä»£ç </sub></p>
      </td>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Infra</b><br><sub>éš”ç¦» ray å’Œ torchrun çš„å·®å¼‚</sub></p>
      </td>
      <td style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Plugin</b><br><sub>ä½¿ç”¨ hub ç»„ä»¶</sub></p>
      </td>
      <td bgcolor="#f6f8fa" style="border: 1px solid #d0d7de; border-radius: 8px; padding: 12px;">
        <p align="center"><b>Hub</b><br><sub>ä¸ HF/MS åº“å¯¹æ¥</sub></p>
      </td>
    </tr>
  </table>
</div>

## Twinkle çš„å¯å®šåˆ¶ç»„ä»¶

åœ¨ Twinkle çš„è®¾è®¡ä¸­ï¼Œtorchrunã€Rayã€HTTP çš„è®­ç»ƒä½¿ç”¨åŒæ ·çš„ APIï¼Œå¹¶å…±äº«ç›¸åŒçš„ç»„ä»¶å’Œè¾“å…¥è¾“å‡ºç»“æ„ã€‚å› æ­¤ï¼Œå…¶å¾ˆå¤šç»„ä»¶å¯ä»¥ç”±å¼€å‘è€…è‡ªå®šä¹‰æ¥å®ç°æ–°çš„ç®—æ³•å¼€å‘ã€‚

ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºæ¨èå®šåˆ¶çš„ç»„ä»¶åˆ—è¡¨ï¼š

| ç»„ä»¶åç§°              | åŸºç±»                                       | è¯´æ˜                                                    |
| --------------------- | ------------------------------------------ | ------------------------------------------------------- |
| æŸå¤±                  | twinkle.loss.Loss                          | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒçš„æŸå¤±å‡½æ•°                              |
| æŒ‡æ ‡                  | twinkle.metric.Metric                      | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒçš„è¯„ä»·ä½“ç³»                              |
| Optimizer/LRScheduler | åŸºäºPyTorch                                | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–å™¨å’ŒLRè¡°å‡å™¨                      |
| è¡¥ä¸                  | twinkle.patch.Patch                        | ç”¨äºä¿®å¤æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„é—®é¢˜                            |
| é¢„å¤„ç†å™¨              | twinkle.preprocessor.Preprocessor          | ç”¨äºå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ˆETLï¼‰ï¼Œå¹¶è¿”å› Template å¯ç”¨çš„æ ‡å‡†æ ¼å¼ |
| è¿‡æ»¤å™¨                | twinkle.preprocessor.Filter                | ç”¨äºå¯¹åŸå§‹æ•°æ®è¿›è¡Œåˆç†æ€§è¿‡æ»¤                            |
| ä»»åŠ¡æ•°æ®å¤„ç†å™¨        | twinkle.processor.InputProcessor           | ç”¨äºå°†æ¨¡å‹è¾“å…¥è½¬æ¢ä¸ºå„ä»»åŠ¡éœ€è¦çš„æ•°æ®ï¼Œå¹¶æ·»åŠ é¢å¤–å­—æ®µ    |
| æ¨¡å‹                  | twinkle.model.TwinkleModel                 | å¤§æ¨¡å‹æœ¬èº«                                              |
| é‡‡æ ·å™¨                | twinkle.sampler.Sampler                    | é‡‡æ ·å™¨ï¼Œä¾‹å¦‚ vLLM                                       |
| å¥–åŠ±                  | twinkle.reward.Reward                      | ç”¨äºå®ç°ä¸åŒ RL è®­ç»ƒçš„å¥–åŠ±                              |
| ä¼˜åŠ¿                  | twinkle.advantage.Advantage                | ç”¨äºå®ç°ä¸åŒ RL è®­ç»ƒçš„ä¼˜åŠ¿ä¼°è®¡                          |
| æ¨¡æ¿                  | twinkle.template.Template                  | ç”¨äºå¤„ç†æ ‡å‡†è¾“å…¥ï¼Œå¹¶è½¬æ¢æˆæ¨¡å‹éœ€è¦çš„ token              |
| æƒé‡åŒæ­¥              | twinkle.checkpoint_engine.CheckpointEngine | ç”¨äº RL è®­ç»ƒä¸­çš„æƒé‡åŒæ­¥                                |

> æœªåœ¨ä¸Šè¡¨ä¸­åˆ—å‡ºçš„ç»„ä»¶ï¼Œå¦‚Datasetã€DataLoaderç­‰ä¹Ÿå¯ä»¥å®ç°å®šåˆ¶ï¼Œåªéœ€è¦è·ŸéšåŸºç±»APIè®¾è®¡å³å¯ã€‚

## DeviceGroup å’Œ DeviceMesh

DeviceGroup å’Œ DeviceMesh æ˜¯ Twinkle æ¶æ„çš„æ ¸å¿ƒã€‚æ‰€æœ‰çš„ä»£ç æ„å»ºå‡åŸºäºè¿™ä¸¤ä¸ªè®¾è®¡ã€‚

```python
import twinkle
from twinkle import DeviceMesh, DeviceGroup
device_group = [
        DeviceGroup(
            name='default',
            ranks=8,
            device_type='cuda',
        )
    ]

device_mesh = DeviceMesh.from_sizes(pp_size=2, tp_size=2, dp_size=2)
twinkle.initialize(mode='ray', nproc_per_node=8, groups=device_group)
```

å½“ device_group å®šä¹‰å®Œæˆåï¼Œéœ€è¦ä½¿ç”¨ `twinkle.initialize` æ¥åˆå§‹åŒ–èµ„æºã€‚

DeviceGroupï¼šå®šä¹‰æœ¬æ¬¡è®­ç»ƒéœ€è¦å¤šå°‘ä¸ªèµ„æºç»„ã€‚å®šä¹‰åï¼Œç»„ä»¶å¯ä»¥é€šè¿‡é€‰æ‹©èµ„æºç»„çš„æ–¹å¼å°†è‡ªå·±è¿è¡Œåœ¨è¿œç«¯ï¼š

```python
from twinkle.model import TransformersModel
model = TransformersModel(model_id='ms://Qwen/Qwen3-4B', remote_group='default', device_mesh=device_mesh)
# æˆ–è€…
from twinkle.model import MegatronModel
model = MegatronModel(model_id='ms://Qwen/Qwen3-4B', remote_group='default', device_mesh=device_mesh)
```

DeviceMesh æŒ‡å®šäº†æ¨¡å‹ç­‰ç»„ä»¶åœ¨èµ„æºç»„ä¸­çš„æ‹“æ‰‘ç»“æ„ã€‚å¯ä»¥ç†è§£ä¸ºå¦‚ä½•è¿›è¡Œå¹¶è¡Œã€‚è¿™ä¼šå½±å“ä¸€ç³»åˆ—çš„æ¡†æ¶å†³ç­–ï¼Œä¾‹å¦‚æ•°æ®è·å–ã€æ•°æ®æ¶ˆè´¹ã€æ•°æ®è¿”å›ç­‰ã€‚

## ä½¿ç”¨æ ·ä¾‹

```python
from peft import LoraConfig
import twinkle
from twinkle import DeviceMesh, DeviceGroup
from twinkle.dataloader import DataLoader
from twinkle.dataset import Dataset, DatasetMeta
from twinkle.model import TransformersModel
from twinkle.preprocessor import SelfCognitionProcessor

device_group = [DeviceGroup(name='default',ranks=8,device_type='cuda')]
device_mesh = DeviceMesh.from_sizes(fsdp_size=4, dp_size=2)
# local for torchrun
twinkle.initialize(mode='ray', groups=device_group, global_device_mesh=device_mesh)


def train():
    # 1000 samples
    dataset = Dataset(dataset_meta=DatasetMeta('ms://swift/self-cognition', data_slice=range(1000)))
    # Set template to prepare encoding
    dataset.set_template('Template', model_id='ms://Qwen/Qwen3-4B')
    # Preprocess the dataset to standard format
    dataset.map(SelfCognitionProcessor('twinkleå¤§æ¨¡å‹', 'ModelScopeç¤¾åŒº'))
    # Encode dataset
    dataset.encode()
    # Global batch size = 8, for GPUs, so 1 sample per GPU
    dataloader = DataLoader(dataset=dataset, batch_size=8, min_batch_size=8)
    # Use a TransformersModel
    model = TransformersModel(model_id='ms://Qwen/Qwen3-4B', remote_group='default')

    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules='all-linear'
    )

    # Add a lora to model, with name `default`
    # Comment this to use full-parameter training
    model.add_adapter_to_model('default', lora_config, gradient_accumulation_steps=2)
    # Add Optimizer for lora `default`
    model.set_optimizer(optimizer_cls='AdamW', lr=1e-4)
    # Add LRScheduler for lora `default`
    model.set_lr_scheduler(scheduler_cls='CosineWarmupScheduler', num_warmup_steps=5,
                           num_training_steps=len(dataloader))
    for step, batch in enumerate(dataloader):
        # Do forward and backward
        model.forward_backward(inputs=batch)
        # Step
        model.clip_grad_and_step()
        if step % 20 == 0:
            # Print metric
            metric = model.calculate_metric(is_training=True)
            print(f'Current is step {step} of {len(dataloader)}, metric: {metric}')
    model.save(f'last-checkpoint')


if __name__ == '__main__':
    train()
```

è¿™æ ·å¯åŠ¨è®­ç»ƒï¼š

```shell
python3 train.py
```

## æ”¯æŒçš„å¤§è¯­è¨€æ¨¡å‹åˆ—è¡¨

| Model Type          | Model ID ä¸¾ä¾‹                                                                                                              | Requires             | Support Megatron | HF Model ID                                                                                                |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------- | -------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------- |
| qwen2 å…¨ç³»åˆ—        | [Qwen/Qwen2-0.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2-0.5B-Instruct)                                             | transformers>=4.37   | âœ”               | [Qwen/Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)                                   |
|                     | [Qwen/Qwen2-72B-Instruct](https://modelscope.cn/models/Qwen/Qwen2-72B-Instruct)                                               | transformers>=4.37   | âœ”               | [Qwen/Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)                                     |
|                     | [Qwen/Qwen2-1.5B](https://modelscope.cn/models/Qwen/Qwen2-1.5B)                                                               | transformers>=4.37   | âœ”               | [Qwen/Qwen2-1.5B](https://huggingface.co/Qwen/Qwen2-1.5B)                                                     |
|                     | [Qwen/Qwen2-7B](https://modelscope.cn/models/Qwen/Qwen2-7B)                                                                   | transformers>=4.37   | âœ”               | [Qwen/Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B)                                                         |
|                     | [Qwen/Qwen2-72B](https://modelscope.cn/models/Qwen/Qwen2-72B)                                                                 | transformers>=4.37   | âœ”               | [Qwen/Qwen2-72B](https://huggingface.co/Qwen/Qwen2-72B)                                                       |
|                     | [Qwen/Qwen2.5-0.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-0.5B-Instruct)                                         | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)                               |
|                     | [Qwen/Qwen2.5-1.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-1.5B-Instruct)                                         | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)                               |
|                     | [Qwen/Qwen2.5-72B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-72B-Instruct)                                           | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)                                 |
|                     | [Qwen/Qwen2.5-0.5B](https://modelscope.cn/models/Qwen/Qwen2.5-0.5B)                                                           | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B)                                                 |
|                     | [Qwen/Qwen2.5-32B](https://modelscope.cn/models/Qwen/Qwen2.5-32B)                                                             | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B)                                                   |
| qwen2_moe å…¨ç³»åˆ—    | [Qwen/Qwen1.5-MoE-A2.7B-Chat](https://modelscope.cn/models/Qwen/Qwen1.5-MoE-A2.7B-Chat)                                       | transformers>=4.40   | âœ”               | [Qwen/Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)                             |
|                     | [Qwen/Qwen1.5-MoE-A2.7B](https://modelscope.cn/models/Qwen/Qwen1.5-MoE-A2.7B)                                                 | transformers>=4.40   | âœ”               | [Qwen/Qwen1.5-MoE-A2.7B](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B)                                       |
| qwen3 å…¨ç³»åˆ—        | [Qwen/Qwen3-0.6B-Base](https://modelscope.cn/models/Qwen/Qwen3-0.6B-Base)                                                     | transformers>=4.51   | âœ”               | [Qwen/Qwen3-0.6B-Base](https://huggingface.co/Qwen/Qwen3-0.6B-Base)                                           |
|                     | [Qwen/Qwen3-14B-Base](https://modelscope.cn/models/Qwen/Qwen3-14B-Base)                                                       | transformers>=4.51   | âœ”               | [Qwen/Qwen3-14B-Base](https://huggingface.co/Qwen/Qwen3-14B-Base)                                             |
|                     | [Qwen/Qwen3-0.6B](https://modelscope.cn/models/Qwen/Qwen3-0.6B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)                                                     |
|                     | [Qwen/Qwen3-1.7B](https://modelscope.cn/models/Qwen/Qwen3-1.7B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B)                                                     |
|                     | [Qwen/Qwen3-32B](https://modelscope.cn/models/Qwen/Qwen2.5-32B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B)                                                       |
| qwen3_moe å…¨ç³»åˆ—    | [Qwen/Qwen3-30B-A3B-Base](https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Base)                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-30B-A3B-Base](https://huggingface.co/Qwen/Qwen3-30B-A3B-Base)                                     |
|                     | [Qwen/Qwen3-30B-A3B](https://modelscope.cn/models/Qwen/Qwen3-30B-A3B)                                                         | transformers>=4.51   | âœ”               | [Qwen/Qwen3-30B-A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)                                               |
|                     | [Qwen/Qwen3-235B-A22B](https://modelscope.cn/models/Qwen/Qwen3-235B-A22B)                                                     | transformers>=4.51   | âœ”               | [Qwen/Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B)                                           |
| chatglm2 å…¨ç³»åˆ—     | [ZhipuAI/chatglm2-6b](https://modelscope.cn/models/ZhipuAI/chatglm2-6b)                                                       | transformers<4.42    | âœ˜               | [zai-org/chatglm2-6b](https://huggingface.co/zai-org/chatglm2-6b)                                             |
|                     | [ZhipuAI/chatglm2-6b-32k](https://modelscope.cn/models/ZhipuAI/chatglm2-6b-32k)                                               | transformers<4.42    | âœ˜               | [zai-org/chatglm2-6b-32k](https://huggingface.co/zai-org/chatglm2-6b-32k)                                     |
| chatglm3 å…¨ç³»åˆ—     | [ZhipuAI/chatglm3-6b](https://modelscope.cn/models/ZhipuAI/chatglm3-6b)                                                       | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b](https://huggingface.co/zai-org/chatglm3-6b)                                             |
|                     | [ZhipuAI/chatglm3-6b-base](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-base)                                             | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-base](https://huggingface.co/zai-org/chatglm3-6b-base)                                   |
|                     | [ZhipuAI/chatglm3-6b-32k](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-32k)                                               | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-32k](https://huggingface.co/zai-org/chatglm3-6b-32k)                                     |
|                     | [ZhipuAI/chatglm3-6b-128k](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-128k)                                             | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-128k](https://huggingface.co/zai-org/chatglm3-6b-128k)                                   |
| chatglm4 å…¨ç³»åˆ—     | [ZhipuAI/glm-4-9b-chat](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat)                                                   | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b-chat](https://huggingface.co/zai-org/glm-4-9b-chat)                                         |
|                     | [ZhipuAI/glm-4-9b](https://modelscope.cn/models/ZhipuAI/glm-4-9b)                                                             | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b](https://huggingface.co/zai-org/glm-4-9b)                                                   |
|                     | [ZhipuAI/glm-4-9b-chat-1m](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat-1m)                                             | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b-chat-1m](https://huggingface.co/zai-org/glm-4-9b-chat-1m)                                   |
|                     | [ZhipuAI/LongWriter-glm4-9b](https://modelscope.cn/models/ZhipuAI/LongWriter-glm4-9b)                                         | transformers>=4.42   | âœ˜               | [zai-org/LongWriter-glm4-9b](https://huggingface.co/zai-org/LongWriter-glm4-9b)                               |
| glm_edge å…¨ç³»åˆ—     | [ZhipuAI/glm-edge-1.5b-chat](https://modelscope.cn/models/ZhipuAI/glm-edge-1.5b-chat)                                         | transformers>=4.46   | âœ˜               | [zai-org/glm-edge-1.5b-chat](https://huggingface.co/zai-org/glm-edge-1.5b-chat)                               |
|                     | [ZhipuAI/glm-edge-4b-chat](https://modelscope.cn/models/ZhipuAI/glm-edge-4b-chat)                                             | transformers>=4.46   | âœ˜               | [zai-org/glm-edge-4b-chat](https://huggingface.co/zai-org/glm-edge-4b-chat)                                   |
| internlm2 å…¨ç³»åˆ—    | [Shanghai_AI_Laboratory/internlm2-1_8b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b)                   | transformers>=4.38   | âœ˜               | [internlm/internlm2-1_8b](https://huggingface.co/internlm/internlm2-1_8b)                                     |
|                     | [Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft) | transformers>=4.38   | âœ˜               | [internlm/internlm2-chat-1_8b-sft](https://huggingface.co/internlm/internlm2-chat-1_8b-sft)                   |
|                     | [Shanghai_AI_Laboratory/internlm2-base-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b)             | transformers>=4.38   | âœ˜               | [internlm/internlm2-base-7b](https://huggingface.co/internlm/internlm2-base-7b)                               |
|                     | [Shanghai_AI_Laboratory/internlm2-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b)                       | transformers>=4.38   | âœ˜               | [internlm/internlm2-7b](https://huggingface.co/internlm/internlm2-7b)                                         |
|                     | [Shanghai_AI_Laboratory/internlm2-chat-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b)             | transformers>=4.38   | âœ˜               | [internlm/internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b)                               |
| deepseek_v1         | [deepseek-ai/deepseek-vl-7b-chat](https://modelscope.cn/models/deepseek-ai/deepseek-vl-7b-chat)                               | transformers>=4.39.4 | âœ”               |                                                                                                            |
|                     | [deepseek-ai/DeepSeek-V2-Lite](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Lite)                                     | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Lite](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite)                           |
|                     | [deepseek-ai/DeepSeek-V2-Lite-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Lite-Chat)                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Lite-Chat](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat)                 |
|                     | [deepseek-ai/DeepSeek-V2](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2)                                               | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2)                                     |
|                     | [deepseek-ai/DeepSeek-V2-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Chat)                                     | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Chat](https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat)                           |
|                     | [deepseek-ai/DeepSeek-V2.5](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2.5)                                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)                                 |
|                     | [deepseek-ai/DeepSeek-Prover-V2-7B](https://modelscope.cn/models/deepseek-ai/DeepSeek-Prover-V2-7B)                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-Prover-V2-7B](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-7B)                 |
|                     | [deepseek-ai/DeepSeek-R1](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1)                                               | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)                                     |
| deepSeek-r1-distill | [deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)           | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)               | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)     |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)             | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-32B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)             | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
