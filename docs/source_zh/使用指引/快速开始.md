# å¿«é€Ÿå¼€å§‹

## âœ¨ Twinkle æ˜¯ä»€ä¹ˆï¼Ÿ

å¤§æ¨¡å‹è®­ç»ƒç»„ä»¶åº“ã€‚åŸºäº PyTorchï¼Œæ›´ç®€æ´ã€æ›´çµæ´»ã€ç”Ÿäº§å°±ç»ªã€‚

ğŸ§© <b>æ¾è€¦åˆæ¶æ„</b> Â· æ ‡å‡†åŒ–æ¥å£<br>
ğŸš€ <b>å¤šè¿è¡Œæ¨¡å¼</b> Â· torchrun / Ray / HTTP<br>
ğŸ”Œ <b>å¤šæ¡†æ¶å…¼å®¹</b> Â· Transformers / Megatron<br>
ğŸ‘¥ <b>å¤šç§Ÿæˆ·æ”¯æŒ</b> Â· å•åŸºåº§æ¨¡å‹éƒ¨ç½²

## Twinkle é€‚é…æ€§

Twinkle å’Œ [ms-swift](https://github.com/modelscope/ms-swift) éƒ½æ˜¯æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œä½†äºŒè€…çš„ç‰¹æ€§æœ‰å¾ˆå¤§ä¸åŒï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©ã€‚

### ä½•æ—¶é€‰æ‹© Twinkle

- å¦‚æœä½ æ˜¯å¤§æ¨¡å‹çš„åˆå­¦è€…ï¼Œå¸Œæœ›æ›´å¥½åœ°äº†è§£æ¨¡å‹æœºåˆ¶å’Œæ¨¡å‹è®­ç»ƒæ–¹æ³•
- å¦‚æœä½ æ˜¯å¤§æ¨¡å‹ç ”ç©¶è€…ï¼Œå¸Œæœ›å®šåˆ¶æ¨¡å‹æˆ–è®­ç»ƒæ–¹æ³•
- å¦‚æœä½ å–„äºç¼–å†™ training loopï¼Œå¸Œæœ›å®šåˆ¶è®­ç»ƒè¿‡ç¨‹
- å¦‚æœä½ å¸Œæœ›æä¾›ä¼ä¸šçº§æˆ–å•†ä¸šåŒ–è®­ç»ƒå¹³å°

### ä½•æ—¶é€‰æ‹©ms-swift

- å¦‚æœä½ ä¸å…³å¿ƒè®­ç»ƒè¿‡ç¨‹ï¼Œå¸Œæœ›ä»…æä¾›æ•°æ®é›†ä¾¿å¯å®Œæˆè®­ç»ƒ
- å¦‚æœä½ éœ€è¦æ›´å¤šçš„æ¨¡å‹æ”¯æŒå’Œæ•°æ®é›†ç§ç±»
- å¦‚æœä½ éœ€è¦Embeddingã€Rerankerã€Classificationç­‰å¤šç§ç±»å‹çš„è®­ç»ƒ
- å¦‚æœä½ éœ€è¦æ¨ç†ã€éƒ¨ç½²ã€é‡åŒ–ç­‰å…¶ä»–èƒ½åŠ›
- å¦‚æœä½ å¯¹æ–°æ¨¡å‹çš„è®­ç»ƒæ”¯æŒæ•æ„Ÿï¼ŒSwift ä¼šä¿è¯ day-0 çš„æ›´æ–°èƒ½åŠ›

## Twinkle çš„å¯å®šåˆ¶ç»„ä»¶

åœ¨ Twinkle çš„è®¾è®¡ä¸­ï¼Œtorchrunã€Rayã€HTTP çš„è®­ç»ƒä½¿ç”¨åŒæ ·çš„ APIï¼Œå¹¶å…±äº«ç›¸åŒçš„ç»„ä»¶å’Œè¾“å…¥è¾“å‡ºç»“æ„ã€‚å› æ­¤ï¼Œå…¶å¾ˆå¤šç»„ä»¶å¯ä»¥ç”±å¼€å‘è€…è‡ªå®šä¹‰æ¥å®ç°æ–°çš„ç®—æ³•å¼€å‘ã€‚

ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºæ¨èå®šåˆ¶çš„ç»„ä»¶åˆ—è¡¨ï¼š

| ç»„ä»¶åç§°              | åŸºç±»                                       | è¯´æ˜                                                    |
| --------------------- | ------------------------------------------ | ------------------------------------------------------- |
| æŸå¤±                  | twinkle.loss.Loss                          | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒçš„æŸå¤±å‡½æ•°                              |
| æŒ‡æ ‡                  | twinkle.metric.Metric                      | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒçš„è¯„ä»·ä½“ç³»                              |
| Optimizer/LRScheduler | åŸºäºPyTorch                                | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–å™¨å’ŒLRè¡°å‡å™¨                      |
| è¡¥ä¸                  | twinkle.patch.Patch                        | ç”¨äºä¿®å¤æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„é—®é¢˜                            |
| é¢„å¤„ç†å™¨              | twinkle.preprocessor.Preprocessor          | ç”¨äºå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ˆETLï¼‰ï¼Œå¹¶è¿”å› Template å¯ç”¨çš„æ ‡å‡†æ ¼å¼ |
| è¿‡æ»¤å™¨                | twinkle.preprocessor.Filter                | ç”¨äºå¯¹åŸå§‹æ•°æ®è¿›è¡Œåˆç†æ€§è¿‡æ»¤                            |
| ä»»åŠ¡æ•°æ®å¤„ç†å™¨        | twinkle.processor.InputProcessor           | ç”¨äºå°†æ¨¡å‹è¾“å…¥è½¬æ¢ä¸ºå„ä»»åŠ¡éœ€è¦çš„æ•°æ®ï¼Œå¹¶æ·»åŠ é¢å¤–å­—æ®µ    |
| æ¨¡å‹                  | twinkle.model.TwinkleModel                 | å¤§æ¨¡å‹æœ¬èº«                                              |
| é‡‡æ ·å™¨                | twinkle.sampler.Sampler                    | é‡‡æ ·å™¨ï¼Œä¾‹å¦‚ vLLM                                       |
| å¥–åŠ±                  | twinkle.reward.Reward                      | ç”¨äºå®ç°ä¸åŒ RL è®­ç»ƒçš„å¥–åŠ±                              |
| ä¼˜åŠ¿                  | twinkle.advantage.Advantage                | ç”¨äºå®ç°ä¸åŒ RL è®­ç»ƒçš„ä¼˜åŠ¿ä¼°è®¡                          |
| æ¨¡æ¿                  | twinkle.template.Template                  | ç”¨äºå¤„ç†æ ‡å‡†è¾“å…¥ï¼Œå¹¶è½¬æ¢æˆæ¨¡å‹éœ€è¦çš„ token              |
| æƒé‡åŒæ­¥              | twinkle.checkpoint_engine.CheckpointEngine | ç”¨äº RL è®­ç»ƒä¸­çš„æƒé‡åŒæ­¥                                |

> æœªåœ¨ä¸Šè¡¨ä¸­åˆ—å‡ºçš„ç»„ä»¶ï¼Œå¦‚Datasetã€DataLoaderç­‰ä¹Ÿå¯ä»¥å®ç°å®šåˆ¶ï¼Œåªéœ€è¦è·ŸéšåŸºç±»APIè®¾è®¡å³å¯ã€‚

## DeviceGroup å’Œ DeviceMesh

DeviceGroup å’Œ DeviceMesh æ˜¯ Twinkle æ¶æ„çš„æ ¸å¿ƒã€‚æ‰€æœ‰çš„ä»£ç æ„å»ºå‡åŸºäºè¿™ä¸¤ä¸ªè®¾è®¡ã€‚

```python
import twinkle
from twinkle import DeviceMesh, DeviceGroup
device_group = [
        DeviceGroup(
            name='default',
            ranks=8,
            device_type='cuda',
        )
    ]

device_mesh = DeviceMesh.from_sizes(pp_size=2, tp_size=2, dp_size=2)
twinkle.initialize(mode='ray', nproc_per_node=8, groups=device_group)
```

å½“ device_group å®šä¹‰å®Œæˆåï¼Œéœ€è¦ä½¿ç”¨ `twinkle.initialize` æ¥åˆå§‹åŒ–èµ„æºã€‚

DeviceGroupï¼šå®šä¹‰æœ¬æ¬¡è®­ç»ƒéœ€è¦å¤šå°‘ä¸ªèµ„æºç»„ã€‚å®šä¹‰åï¼Œç»„ä»¶å¯ä»¥é€šè¿‡é€‰æ‹©èµ„æºç»„çš„æ–¹å¼å°†è‡ªå·±è¿è¡Œåœ¨è¿œç«¯ï¼š

```python
from twinkle.model import TransformersModel
model = TransformersModel(model_id='ms://Qwen/Qwen2.5-7B-Instruct', remote_group='default', device_mesh=device_mesh)
# æˆ–è€…
from twinkle.model import MegatronModel
model = MegatronModel(model_id='ms://Qwen/Qwen2.5-7B-Instruct', remote_group='default', device_mesh=device_mesh)
```

DeviceMesh æŒ‡å®šäº†æ¨¡å‹ç­‰ç»„ä»¶åœ¨èµ„æºç»„ä¸­çš„æ‹“æ‰‘ç»“æ„ã€‚å¯ä»¥ç†è§£ä¸ºå¦‚ä½•è¿›è¡Œå¹¶è¡Œã€‚è¿™ä¼šå½±å“ä¸€ç³»åˆ—çš„æ¡†æ¶å†³ç­–ï¼Œä¾‹å¦‚æ•°æ®è·å–ã€æ•°æ®æ¶ˆè´¹ã€æ•°æ®è¿”å›ç­‰ã€‚

## ä½¿ç”¨æ ·ä¾‹

```python
from peft import LoraConfig
import twinkle
from twinkle import DeviceMesh, DeviceGroup
from twinkle.dataloader import DataLoader
from twinkle.dataset import Dataset, DatasetMeta
from twinkle.model import TransformersModel
from twinkle.preprocessor import SelfCognitionProcessor

device_group = [DeviceGroup(name='default',ranks=8,device_type='cuda')]
device_mesh = DeviceMesh.from_sizes(fsdp_size=4, dp_size=2)
# local for torchrun
twinkle.initialize(mode='ray', groups=device_group, global_device_mesh=device_mesh)


def train():
    # 1000 samples
    dataset = Dataset(dataset_meta=DatasetMeta('ms://swift/self-cognition', data_slice=range(1000)))
    # Set template to prepare encoding
    dataset.set_template('Template', model_id='ms://Qwen/Qwen2.5-7B-Instruct')
    # Preprocess the dataset to standard format
    dataset.map(SelfCognitionProcessor('twinkleå¤§æ¨¡å‹', 'ModelScopeç¤¾åŒº'))
    # Encode dataset
    dataset.encode()
    # Global batch size = 8, for GPUs, so 1 sample per GPU
    dataloader = DataLoader(dataset=dataset, batch_size=8, min_batch_size=8)
    # Use a TransformersModel
    model = TransformersModel(model_id='ms://Qwen/Qwen2.5-7B-Instruct', remote_group='default')

    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules='all-linear'
    )

    # Add a lora to model, with name `default`
    # Comment this to use full-parameter training
    model.add_adapter_to_model('default', lora_config, gradient_accumulation_steps=2)
    # Add Optimizer for lora `default`
    model.set_optimizer(optimizer_cls='AdamW', lr=1e-4)
    # Add LRScheduler for lora `default`
    model.set_lr_scheduler(scheduler_cls='CosineWarmupScheduler', num_warmup_steps=5,
                           num_training_steps=len(dataloader))
    for step, batch in enumerate(dataloader):
        # Do forward and backward
        model.forward_backward(inputs=batch)
        # Step
        model.clip_grad_and_step()
        if step % 20 == 0:
            # Print metric
            metric = model.calculate_metric(is_training=True)
            print(f'Current is step {step} of {len(dataloader)}, metric: {metric}')
    model.save(f'last-checkpoint')


if __name__ == '__main__':
    train()
```

è¿™æ ·å¯åŠ¨è®­ç»ƒï¼š

```shell
python3 train.py
```

## æ”¯æŒçš„å¤§è¯­è¨€æ¨¡å‹åˆ—è¡¨

| Model Type          | Model ID ä¸¾ä¾‹                                                                                                              | Requires             | Support Megatron | HF Model ID                                                                                                |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------- | -------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------- |
| qwen2 å…¨ç³»åˆ—        | [Qwen/Qwen2-0.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2-0.5B-Instruct)                                             | transformers>=4.37   | âœ”               | [Qwen/Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)                                   |
|                     | [Qwen/Qwen2-72B-Instruct](https://modelscope.cn/models/Qwen/Qwen2-72B-Instruct)                                               | transformers>=4.37   | âœ”               | [Qwen/Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)                                     |
|                     | [Qwen/Qwen2-1.5B](https://modelscope.cn/models/Qwen/Qwen2-1.5B)                                                               | transformers>=4.37   | âœ”               | [Qwen/Qwen2-1.5B](https://huggingface.co/Qwen/Qwen2-1.5B)                                                     |
|                     | [Qwen/Qwen2-7B](https://modelscope.cn/models/Qwen/Qwen2-7B)                                                                   | transformers>=4.37   | âœ”               | [Qwen/Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B)                                                         |
|                     | [Qwen/Qwen2-72B](https://modelscope.cn/models/Qwen/Qwen2-72B)                                                                 | transformers>=4.37   | âœ”               | [Qwen/Qwen2-72B](https://huggingface.co/Qwen/Qwen2-72B)                                                       |
|                     | [Qwen/Qwen2.5-0.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-0.5B-Instruct)                                         | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)                               |
|                     | [Qwen/Qwen2.5-1.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-1.5B-Instruct)                                         | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)                               |
|                     | [Qwen/Qwen2.5-72B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-72B-Instruct)                                           | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)                                 |
|                     | [Qwen/Qwen2.5-0.5B](https://modelscope.cn/models/Qwen/Qwen2.5-0.5B)                                                           | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B)                                                 |
|                     | [Qwen/Qwen2.5-32B](https://modelscope.cn/models/Qwen/Qwen2.5-32B)                                                             | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B)                                                   |
| qwen2_moe å…¨ç³»åˆ—    | [Qwen/Qwen1.5-MoE-A2.7B-Chat](https://modelscope.cn/models/Qwen/Qwen1.5-MoE-A2.7B-Chat)                                       | transformers>=4.40   | âœ”               | [Qwen/Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)                             |
|                     | [Qwen/Qwen1.5-MoE-A2.7B](https://modelscope.cn/models/Qwen/Qwen1.5-MoE-A2.7B)                                                 | transformers>=4.40   | âœ”               | [Qwen/Qwen1.5-MoE-A2.7B](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B)                                       |
| qwen3 å…¨ç³»åˆ—        | [Qwen/Qwen3-0.6B-Base](https://modelscope.cn/models/Qwen/Qwen3-0.6B-Base)                                                     | transformers>=4.51   | âœ”               | [Qwen/Qwen3-0.6B-Base](https://huggingface.co/Qwen/Qwen3-0.6B-Base)                                           |
|                     | [Qwen/Qwen3-14B-Base](https://modelscope.cn/models/Qwen/Qwen3-14B-Base)                                                       | transformers>=4.51   | âœ”               | [Qwen/Qwen3-14B-Base](https://huggingface.co/Qwen/Qwen3-14B-Base)                                             |
|                     | [Qwen/Qwen3-0.6B](https://modelscope.cn/models/Qwen/Qwen3-0.6B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)                                                     |
|                     | [Qwen/Qwen3-1.7B](https://modelscope.cn/models/Qwen/Qwen3-1.7B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B)                                                     |
|                     | [Qwen/Qwen3-32B](https://modelscope.cn/models/Qwen/Qwen2.5-32B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B)                                                       |
| qwen3_moe å…¨ç³»åˆ—    | [Qwen/Qwen3-30B-A3B-Base](https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Base)                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-30B-A3B-Base](https://huggingface.co/Qwen/Qwen3-30B-A3B-Base)                                     |
|                     | [Qwen/Qwen3-30B-A3B](https://modelscope.cn/models/Qwen/Qwen3-30B-A3B)                                                         | transformers>=4.51   | âœ”               | [Qwen/Qwen3-30B-A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)                                               |
|                     | [Qwen/Qwen3-235B-A22B](https://modelscope.cn/models/Qwen/Qwen3-235B-A22B)                                                     | transformers>=4.51   | âœ”               | [Qwen/Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B)                                           |
| chatglm2 å…¨ç³»åˆ—     | [ZhipuAI/chatglm2-6b](https://modelscope.cn/models/ZhipuAI/chatglm2-6b)                                                       | transformers<4.42    | âœ˜               | [zai-org/chatglm2-6b](https://huggingface.co/zai-org/chatglm2-6b)                                             |
|                     | [ZhipuAI/chatglm2-6b-32k](https://modelscope.cn/models/ZhipuAI/chatglm2-6b-32k)                                               | transformers<4.42    | âœ˜               | [zai-org/chatglm2-6b-32k](https://huggingface.co/zai-org/chatglm2-6b-32k)                                     |
| chatglm3 å…¨ç³»åˆ—     | [ZhipuAI/chatglm3-6b](https://modelscope.cn/models/ZhipuAI/chatglm3-6b)                                                       | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b](https://huggingface.co/zai-org/chatglm3-6b)                                             |
|                     | [ZhipuAI/chatglm3-6b-base](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-base)                                             | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-base](https://huggingface.co/zai-org/chatglm3-6b-base)                                   |
|                     | [ZhipuAI/chatglm3-6b-32k](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-32k)                                               | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-32k](https://huggingface.co/zai-org/chatglm3-6b-32k)                                     |
|                     | [ZhipuAI/chatglm3-6b-128k](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-128k)                                             | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-128k](https://huggingface.co/zai-org/chatglm3-6b-128k)                                   |
| chatglm4 å…¨ç³»åˆ—     | [ZhipuAI/glm-4-9b-chat](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat)                                                   | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b-chat](https://huggingface.co/zai-org/glm-4-9b-chat)                                         |
|                     | [ZhipuAI/glm-4-9b](https://modelscope.cn/models/ZhipuAI/glm-4-9b)                                                             | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b](https://huggingface.co/zai-org/glm-4-9b)                                                   |
|                     | [ZhipuAI/glm-4-9b-chat-1m](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat-1m)                                             | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b-chat-1m](https://huggingface.co/zai-org/glm-4-9b-chat-1m)                                   |
|                     | [ZhipuAI/LongWriter-glm4-9b](https://modelscope.cn/models/ZhipuAI/LongWriter-glm4-9b)                                         | transformers>=4.42   | âœ˜               | [zai-org/LongWriter-glm4-9b](https://huggingface.co/zai-org/LongWriter-glm4-9b)                               |
| glm_edge å…¨ç³»åˆ—     | [ZhipuAI/glm-edge-1.5b-chat](https://modelscope.cn/models/ZhipuAI/glm-edge-1.5b-chat)                                         | transformers>=4.46   | âœ˜               | [zai-org/glm-edge-1.5b-chat](https://huggingface.co/zai-org/glm-edge-1.5b-chat)                               |
|                     | [ZhipuAI/glm-edge-4b-chat](https://modelscope.cn/models/ZhipuAI/glm-edge-4b-chat)                                             | transformers>=4.46   | âœ˜               | [zai-org/glm-edge-4b-chat](https://huggingface.co/zai-org/glm-edge-4b-chat)                                   |
| internlm2 å…¨ç³»åˆ—    | [Shanghai_AI_Laboratory/internlm2-1_8b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b)                   | transformers>=4.38   | âœ˜               | [internlm/internlm2-1_8b](https://huggingface.co/internlm/internlm2-1_8b)                                     |
|                     | [Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft) | transformers>=4.38   | âœ˜               | [internlm/internlm2-chat-1_8b-sft](https://huggingface.co/internlm/internlm2-chat-1_8b-sft)                   |
|                     | [Shanghai_AI_Laboratory/internlm2-base-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b)             | transformers>=4.38   | âœ˜               | [internlm/internlm2-base-7b](https://huggingface.co/internlm/internlm2-base-7b)                               |
|                     | [Shanghai_AI_Laboratory/internlm2-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b)                       | transformers>=4.38   | âœ˜               | [internlm/internlm2-7b](https://huggingface.co/internlm/internlm2-7b)                                         |
|                     | [Shanghai_AI_Laboratory/internlm2-chat-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b)             | transformers>=4.38   | âœ˜               | [internlm/internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b)                               |
| deepseek_v1         | [deepseek-ai/deepseek-vl-7b-chat](https://modelscope.cn/models/deepseek-ai/deepseek-vl-7b-chat)                               | transformers>=4.39.4 | âœ”               |                                                                                                            |
|                     | [deepseek-ai/DeepSeek-V2-Lite](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Lite)                                     | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Lite](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite)                           |
|                     | [deepseek-ai/DeepSeek-V2-Lite-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Lite-Chat)                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Lite-Chat](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat)                 |
|                     | [deepseek-ai/DeepSeek-V2](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2)                                               | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2)                                     |
|                     | [deepseek-ai/DeepSeek-V2-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Chat)                                     | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Chat](https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat)                           |
|                     | [deepseek-ai/DeepSeek-V2.5](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2.5)                                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)                                 |
|                     | [deepseek-ai/DeepSeek-Prover-V2-7B](https://modelscope.cn/models/deepseek-ai/DeepSeek-Prover-V2-7B)                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-Prover-V2-7B](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-7B)                 |
|                     | [deepseek-ai/DeepSeek-R1](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1)                                               | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)                                     |
| deepSeek-r1-distill | [deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)           | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)               | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)     |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)             | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-32B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)             | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
