# 服务端和客户端

Twinkle 提供了完整的 HTTP Server/Client 架构，支持将模型部署为服务，并通过客户端远程调用完成训练、推理等任务。这种架构将**模型承载（Server 端）**和**训练逻辑（Client 端）**解耦，使得多个用户可以共享同一个基座模型进行训练。

## 核心概念

- **Server 端**：基于 Ray Serve 部署，承载模型权重和推理/训练计算。Server 负责管理模型加载、前向/反向传播、权重保存、采样推理等。
- **Client 端**：在本地运行，负责数据准备、训练循环编排、超参配置等。Client 通过 HTTP 与 Server 通信，发送数据和指令。

### 两种 Server 模式

Twinkle Server 支持两种协议模式：

| 模式 | server_type | 说明 |
|------|------------|------|
| **Twinkle Server** | `twinkle` | 原生 Twinkle 协议，搭配 `twinkle_client` 使用，API 更简洁 |
| **Tinker 兼容 Server** | `tinker` | 兼容 Tinker 协议，搭配 `init_tinker_compat_client` 使用，可复用已有 Tinker 训练代码 |

### 两种模型后端

无论哪种 Server 模式，模型加载均支持两种后端：

| 后端 | use_megatron | 说明 |
|------|-------------|------|
| **Transformers** | `false` | 基于 HuggingFace Transformers，适用于大多数场景 |
| **Megatron** | `true` | 基于 Megatron-LM，适用于超大规模模型训练，支持更高效的并行策略 |

### 两种 Client 模式

| Client | 初始化方式 | 说明 |
|--------|---------|------|
| **Twinkle Client** | `init_twinkle_client` | 原生客户端，将 `from twinkle import` 改为 `from twinkle_client import` 即可将本地训练代码迁移为远端调用 |
| **Tinker 兼容 Client** | `init_tinker_compat_client` | 对 Tinker SDK 进行 patch，使已有 Tinker 训练代码可直接复用 |

## 如何选择

### Server 模式选择

| 场景 | 推荐 |
|------|------|
| 全新项目，使用 Twinkle 体系 | Twinkle Server (`server_type: twinkle`) |
| 已有 Tinker 训练代码，希望迁移到 Twinkle | Tinker 兼容 Server (`server_type: tinker`) |
| 需要推理采样功能 | Tinker 兼容 Server（内置 Sampler 支持） |

### Client 模式选择

| 场景 | 推荐 |
|------|------|
| 已有 Twinkle 本地训练代码，希望改为远端 | Twinkle Client — 仅需改 import 路径 |
| 已有 Tinker 训练代码，希望复用 | Tinker 兼容 Client — 仅需初始化 patch |
| 全新项目 | Twinkle Client — API 更简洁 |

### 模型后端选择

| 场景 | 推荐 |
|------|------|
| 7B/14B 等中小规模模型 | Transformers 后端 |
| 超大规模模型，需要高级并行策略 | Megatron 后端 |
| 快速实验和原型验证 | Transformers 后端 |

## Cookbook 参考

完整的可运行示例位于 `cookbook/client/` 目录：

```
cookbook/client/
├── twinkle/                    # Twinkle 原生协议示例
│   ├── transformer/            # Transformers 后端
│   │   ├── server.py           # 启动脚本
│   │   ├── server_config.yaml  # 配置文件
│   │   └── lora.py             # LoRA 训练客户端
│   └── megatron/               # Megatron 后端
│       ├── server.py
│       ├── server_config.yaml
│       └── lora.py
└── tinker/                     # Tinker 兼容协议示例
    ├── transformer/            # Transformers 后端
    │   ├── server.py
    │   ├── server_config.yaml
    │   ├── lora.py             # LoRA 训练
    │   ├── sample.py           # 推理采样
    │   └── self_congnition.py  # 自我认知训练+评估
    └── megatron/               # Megatron 后端
        ├── server.py
        ├── server_config.yaml
        └── lora.py
```

运行步骤：

```bash
# 1. 先启动 Server
python cookbook/client/twinkle/transformer/server.py

# 2. 在另一个终端运行 Client
python cookbook/client/twinkle/transformer/lora.py
```
