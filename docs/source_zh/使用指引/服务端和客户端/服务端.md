# 服务端（Server）

## Ray 集群配置

在启动 Server 之前，**必须先启动并配置 Ray 节点**。只有正确配置了 Ray 节点后，Server 才能正确分配和占用资源（GPU、CPU 等）。

### 启动 Ray 节点

Ray 集群由多个节点（Node）组成，每个节点可以配置不同的资源。启动步骤如下：

#### 1. 启动 Head 节点（第一个 GPU 节点）

```bash
# 停止已有的 Ray 集群（如果有）
ray stop

# 启动 Head 节点，使用 GPU 0-3，共 4 个 GPU
CUDA_VISIBLE_DEVICES=0,1,2,3 ray start --head --num-gpus=4 --port=6379
```

#### 2. 启动 Worker 节点

```bash
# 第二个 GPU 节点，使用 GPU 4-7，共 4 个 GPU
CUDA_VISIBLE_DEVICES=4,5,6,7 ray start --address=10.28.252.9:6379 --num-gpus=4

# CPU 节点（用于运行 Processor 等 CPU 任务）
ray start --address=10.28.252.9:6379 --num-gpus=0
```

**说明：**
- `--head`：标记此节点为 Head 节点（集群的主节点）
- `--port=6379`：Head 节点监听端口
- `--address=<IP>:<PORT>`：Worker 节点连接到 Head 节点的地址
- `--num-gpus=N`：该节点可用的 GPU 数量
- `CUDA_VISIBLE_DEVICES`：限制该节点可见的 GPU 设备

#### 3. 完整示例：3 节点集群

```bash
# 停止旧集群并启动新集群
ray stop && \
CUDA_VISIBLE_DEVICES=0,1,2,3 ray start --head --num-gpus=4 --port=6379 && \
CUDA_VISIBLE_DEVICES=4,5,6,7 ray start --address=10.28.252.9:6379 --num-gpus=4 && \
ray start --address=10.28.252.9:6379 --num-gpus=0
```

此配置启动了 3 个节点：
- **Node 0**（Head）：4 个 GPU（卡 0-3）
- **Node 1**（Worker）：4 个 GPU（卡 4-7）
- **Node 2**（Worker）：纯 CPU 节点

#### 4. 设置环境变量

在启动 Server 之前，需要设置以下环境变量：

```bash
export DEVICE_COUNT_PER_PHYSICAL_NODE=8  # 指定每台物理机上的 GPU 总数
export TWINKLE_TRUST_REMOTE_CODE=0       # 是否信任远程代码（安全考虑）
```

> **重要提示**：`DEVICE_COUNT_PER_PHYSICAL_NODE` 必须设置为机器上实际的物理 GPU 数量，这对于正确解析 `ranks` 配置至关重要。

### YAML 配置中的 Node Rank

在 YAML 配置文件中，**每个组件需要占用一个独立的 Node**。

**示例配置：**

```yaml
applications:
  # 模型服务占用 GPU 0-3（物理卡号）
  - name: models-Qwen2.5-7B-Instruct
    route_prefix: /models/Qwen/Qwen2.5-7B-Instruct
    import_path: model
    args:
      nproc_per_node: 4
      device_group:
        name: model
        ranks: [0, 1, 2, 3]    # 物理 GPU 卡号
        device_type: cuda
      device_mesh:
        device_type: cuda
        dp_size: 4             # 数据并行大小
        # tp_size: 1           # 张量并行大小（可选）
        # pp_size: 1           # 流水线并行大小（可选）
        # ep_size: 1           # 专家并行大小（可选）

  # Sampler 服务占用 GPU 4-5（物理卡号）
  - name: sampler-Qwen2.5-7B-Instruct
    route_prefix: /sampler/Qwen/Qwen2.5-7B-Instruct
    import_path: sampler
    args:
      nproc_per_node: 2
      device_group:
        name: sampler
        ranks: [4, 5]          # 物理 GPU 卡号 4-5
        device_type: cuda
      device_mesh:
        device_type: cuda
        dp_size: 2             # 数据并行大小

  # Processor 服务占用 CPU
  - name: processor
    route_prefix: /processors
    import_path: processor
    args:
      ncpu_proc_per_node: 4
      device_group:
        name: processor
        ranks: 0               # CPU 编号
        device_type: CPU
      device_mesh:
        device_type: CPU
        dp_size: 4             # 数据并行大小
```
**重要提示：**
- `ranks` 配置使用**物理 GPU 卡号**，直接对应机器上的实际 GPU 设备
- `device_mesh` 配置使用 `dp_size`、`tp_size`、`pp_size`、`ep_size` 等参数替代原来的 `mesh` 和 `mesh_dim_names`
- 必须设置环境变量 `DEVICE_COUNT_PER_PHYSICAL_NODE` 来告知系统每台机器的物理 GPU 总数
- 不同组件会自动分配到不同的 Node 上
- Ray 会根据资源需求（`ray_actor_options` 中的 `num_gpus`、`num_cpus`）自动调度到合适的 Node

## 启动方式

Server 统一通过 `launch_server` 函数或 CLI 命令启动，配合 YAML 配置文件。

### 方式一：Python 脚本启动

```python
# server.py
import os
from twinkle.server import launch_server

# 获取配置文件路径（与脚本同目录的 server_config.yaml）
file_dir = os.path.abspath(os.path.dirname(__file__))
config_path = os.path.join(file_dir, 'server_config.yaml')

# 启动服务，此调用将阻塞直到服务关闭
launch_server(config_path=config_path)
```

### 方式二：命令行启动

```bash
# 启动 Twinkle 原生 Server
python -m twinkle.server --config server_config.yaml

# 启动 Tinker 兼容 Server
python -m twinkle.server --config server_config.yaml --server-type tinker
```

CLI 支持的参数：

| 参数 | 说明 | 默认值 |
|------|------|-------|
| `-c, --config` | YAML 配置文件路径（必须） | — |
| `-t, --server-type` | Server 模式：`twinkle` 或 `tinker` | `twinkle` |
| `--namespace` | Ray 命名空间 | tinker 模式默认 `twinkle_cluster` |
| `--no-wait` | 不阻塞等待（守护模式） | `False` |
| `--log-level` | 日志级别 | `INFO` |

## YAML 配置详解

配置文件定义了 Server 的完整部署方案，包括 HTTP 监听、应用组件和资源分配。

### Twinkle Server + Transformers 后端

```yaml
# server_config.yaml — Twinkle 原生协议 + Transformers 后端

# 协议类型：twinkle 原生协议
server_type: twinkle

# HTTP 代理位置：EveryNode 表示每个 Ray 节点运行一个代理（多节点场景推荐）
proxy_location: EveryNode

# HTTP 监听配置
http_options:
  host: 0.0.0.0        # 监听所有网络接口
  port: 8000            # 服务端口号

# 应用列表：每个条目定义一个部署在 Server 上的服务组件
applications:

  # 1. TwinkleServer：中央管理服务
  # 负责处理客户端连接、训练运行跟踪、检查点管理等
  - name: server
    route_prefix: /server          # API 路径前缀
    import_path: server            # 内置组件标识
    args:                          # 无额外参数
    deployments:
      - name: TwinkleServer
        autoscaling_config:
          min_replicas: 1                # 最小副本数
          max_replicas: 1                # 最大副本数
          target_ongoing_requests: 128   # 每副本目标并发请求数
        ray_actor_options:
          num_cpus: 0.1                  # 此 Actor 分配的 CPU 资源

  # 2. Model 服务：承载基座模型
  # 执行前向传播、反向传播等训练计算
  - name: models-Qwen2.5-7B-Instruct
    route_prefix: /models/Qwen/Qwen2.5-7B-Instruct   # 模型的 REST 路径
    import_path: model
    args:
      use_megatron: false                              # 使用 Transformers 后端
      model_id: "ms://Qwen/Qwen2.5-7B-Instruct"      # ModelScope 模型标识
      adapter_config:                                  # LoRA 适配器配置
        per_token_adapter_limit: 30   # 同时可激活的最大 LoRA 数量
        adapter_timeout: 1800         # 空闲适配器超时卸载时间（秒）
      nproc_per_node: 2               # 每节点 GPU 进程数
      device_group:                   # 逻辑设备组
        name: model
        ranks: [0, 1]                 # 物理 GPU 卡号
        device_type: cuda
      device_mesh:                    # 分布式训练网格
        device_type: cuda
        dp_size: 2                    # 数据并行大小
        # tp_size: 1                  # 张量并行大小（可选）
        # pp_size: 1                  # 流水线并行大小（可选）
    deployments:
      - name: ModelManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 16
        ray_actor_options:
          num_cpus: 0.1

  # 3. Processor 服务：数据预处理
  # 在 CPU 上执行 tokenization、模板转换等预处理任务
  - name: processor
    route_prefix: /processors
    import_path: processor
    args:
      nproc_per_node: 2               # 每节点处理器 worker 数
      ncpu_proc_per_node: 2           # 每节点 CPU 进程数
      device_group:
        name: model
        ranks: 2
        device_type: CPU
      device_mesh:
        device_type: CPU
        dp_size: 2                    # 数据并行大小
    deployments:
      - name: ProcessorManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 128
        ray_actor_options:
          num_cpus: 0.1
```

### Twinkle Server + Megatron 后端

与 Transformers 后端的区别仅在 Model 服务的 `use_megatron` 参数：

```yaml
  # Model 服务 — Megatron 后端
  - name: models-Qwen2.5-7B-Instruct
    route_prefix: /models/Qwen/Qwen2.5-7B-Instruct
    import_path: model
    args:
      use_megatron: true                               # 使用 Megatron-LM 后端
      model_id: "ms://Qwen/Qwen2.5-7B-Instruct"
      nproc_per_node: 2
      device_group:
        name: model
        ranks: [0, 1]
        device_type: cuda
      device_mesh:
        device_type: cuda
        dp_size: 2                    # 数据并行大小
```

> **注意**：Megatron 后端不需要 `adapter_config`（LoRA 适配器管理由 Megatron 内部处理）。

### Tinker 兼容 Server 配置

Tinker 兼容模式的主要区别：
- `server_type` 设为 `tinker`
- `route_prefix` 使用 `/api/v1` 前缀（Tinker 协议规范）
- 可额外配置 Sampler 服务（用于推理采样）

```yaml
# server_config.yaml — Tinker 兼容协议

server_type: tinker

proxy_location: EveryNode

http_options:
  host: 0.0.0.0
  port: 8000

applications:

  # 1. TinkerCompatServer：中央 API 服务
  - name: server
    route_prefix: /api/v1              # Tinker 协议 API 前缀
    import_path: server
    args:
    deployments:
      - name: TinkerCompatServer
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 128
        ray_actor_options:
          num_cpus: 0.1

  # 2. Model 服务（Megatron 后端示例）
  - name: models-Qwen2.5-0.5B-Instruct
    route_prefix: /api/v1/model/Qwen/Qwen2.5-0.5B-Instruct
    import_path: model
    args:
      use_megatron: true
      model_id: "ms://Qwen/Qwen2.5-0.5B-Instruct"
      nproc_per_node: 2
      device_group:
        name: model
        ranks: [0, 1]
        device_type: cuda
      device_mesh:
        device_type: cuda
        dp_size: 2                    # 数据并行大小
    deployments:
      - name: ModelManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 16
        ray_actor_options:
          num_cpus: 0.1
          runtime_env:
            env_vars:
              DEVICE_COUNT_PER_PHYSICAL_NODE: "8"  # 每台机器的物理 GPU 总数

  # 3. Sampler 服务（可选，用于推理采样）
  - name: sampler-Qwen2.5-0.5B-Instruct
    route_prefix: /api/v1/sampler/Qwen/Qwen2.5-0.5B-Instruct
    import_path: sampler
    args:
      model_id: "ms://Qwen/Qwen2.5-0.5B-Instruct"
      nproc_per_node: 1
      sampler_type: vllm              # 推理引擎：vllm（高性能）或 torch
      engine_args:                    # vLLM 引擎参数
        max_model_len: 4096           # 最大序列长度
        gpu_memory_utilization: 0.5   # GPU 显存使用比例
        enable_lora: true             # 支持推理时加载 LoRA
      device_group:
        name: sampler
        ranks: [0]
        device_type: cuda
      device_mesh:
        device_type: cuda
        dp_size: 1                    # 数据并行大小
    deployments:
      - name: SamplerManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 16
        ray_actor_options:
          num_cpus: 0.1
          num_gpus: 1                 # Sampler 需要独立 GPU
          runtime_env:
            env_vars:
              DEVICE_COUNT_PER_PHYSICAL_NODE: "8"  # 每台机器的物理 GPU 总数
```

## 配置项说明

### 应用组件（import_path）

| import_path | Twinkle 模式 | Tinker 模式 | 说明 |
|-------------|-------------|------------|------|
| `server` | ✅ | ✅ | 中央管理服务，处理训练运行和检查点 |
| `model` | ✅ | ✅ | 模型服务，承载基座模型进行训练 |
| `processor` | ✅ | ❌ | 数据预处理服务（仅 Twinkle 模式，Tinker 模式需在本地处理） |
| `sampler` | ✅ | ✅ | 推理采样服务 |

### device_group 与 device_mesh

- **device_group**：定义逻辑设备组，指定使用哪些 GPU 卡
- **device_mesh**：定义分布式训练网格，控制并行策略

```yaml
device_group:
  name: model          # 设备组名称
  ranks: [0, 1]        # 物理 GPU 卡号列表
  device_type: cuda     # 设备类型：cuda / CPU

device_mesh:
  device_type: cuda
  dp_size: 2           # 数据并行大小
  # tp_size: 1         # 张量并行大小（可选）
  # pp_size: 1         # 流水线并行大小（可选）
  # ep_size: 1         # 专家并行大小（可选）
```

**重要配置参数说明：**

| 参数 | 类型 | 说明 |
|------|------|------|
| `ranks` | list[int] | **物理 GPU 卡号**，直接对应机器上的实际 GPU 设备 |
| `dp_size` | int | 数据并行大小 |
| `tp_size` | int (可选) | 张量并行大小 |
| `pp_size` | int (可选) | 流水线并行大小 |
| `ep_size` | int (可选) | 专家并行大小（用于 MoE 模型） |

**环境变量：**

```bash
export DEVICE_COUNT_PER_PHYSICAL_NODE=8  # 每台物理机上的 GPU 总数（必须设置）
export TWINKLE_TRUST_REMOTE_CODE=0       # 是否信任远程代码
```
