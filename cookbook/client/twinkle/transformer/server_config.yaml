# Twinkle Server Configuration - Transformers Backend

# Server protocol type: "twinkle" for the native Twinkle client protocol
server_type: twinkle

# proxy_location: determines where the HTTP proxy runs.
# "EveryNode" means each Ray node runs its own proxy (good for multi-node).
proxy_location: EveryNode

# HTTP listener settings
http_options:
  host: 0.0.0.0        # Listen on all network interfaces
  port: 8000            # Port number for the server

# Applications: each entry defines a service component deployed on the server
applications:

  # 1. TwinkleServer - The central management server
  #    Handles client connections, training run tracking, checkpoint listing.
  - name: server
    route_prefix: /server          # API endpoint prefix
    import_path: server            # Python module to import
    args:
      server_config:
        per_token_model_limit: 3      # Maximum number of models (adapters) per token (server-globally enforced)
    deployments:
      - name: TwinkleServer
        autoscaling_config:
          min_replicas: 1                # Minimum number of replicas
          max_replicas: 1                # Maximum number of replicas
          target_ongoing_requests: 128   # Target concurrent requests per replica
        ray_actor_options:
          num_cpus: 0.1                  # CPU resources allocated to this actor

  # 2. Model Service - Hosts the base model for training
  #    This is the actual model worker that performs forward/backward passes.
  - name: models-Qwen3-4B
    route_prefix: /models/Qwen/Qwen3-4B   # REST path for this model
    import_path: model
    args:
      use_megatron: false                              # Use HuggingFace Transformers (not Megatron)
      model_id: "ms://Qwen/Qwen3-4B"     # ModelScope model identifier to load
      adapter_config:
        adapter_timeout: 1800         # Seconds before an idle adapter is unloaded
      nproc_per_node: 2               # Number of GPU processes per node
      device_group:                   # Logical device group for this model
        name: model
        ranks: 2                      # Number of GPUs to use
        device_type: cuda
      device_mesh:                    # Distributed training mesh configuration
        device_type: cuda
        dp_size: 2                    # Mesh dimension names: 'dp' = data parallel
    deployments:
      - name: ModelManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 16
        ray_actor_options:
          num_cpus: 0.1
          runtime_env:
            env_vars:
              TWINKLE_TRUST_REMOTE_CODE: "0"

  # 3. Processor Service - Handles data preprocessing on CPU
  #    Runs tokenization, template application, and other CPU-bound tasks.
  - name: processor
    route_prefix: /processors
    import_path: processor
    args:
      nproc_per_node: 2               # Number of processor workers per node
      ncpu_proc_per_node: 2           # Number of CPU processes per node
      device_group:
        name: model
        ranks: 2                      # Number of CPU workers to use
        device_type: CPU
      device_mesh:
        device_type: CPU
        dp_size: 2                    # Data parallel size
    deployments:
      - name: ProcessorManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 128
        ray_actor_options:
          num_cpus: 0.1
          runtime_env:
            env_vars:
              TWINKLE_TRUST_REMOTE_CODE: "0"

  # 4. Sampler Service - Handles text generation inference
  #    Uses vLLM for efficient batched generation with optional LoRA adapters.
  - name: sampler-Qwen3-4B
    route_prefix: /samplers/Qwen/Qwen3-4B   # REST path for this sampler
    import_path: sampler
    args:
      model_id: "ms://Qwen/Qwen3-4B"        # ModelScope model identifier to load
      sampler_type: vllm                                 # Sampler backend (vllm or torch)
      nproc_per_node: 2                                  # Number of GPU processes per node
      engine_args:                                       # vLLM engine configuration
        gpu_memory_utilization: 0.4
        max_model_len: 1024
      adapter_config:                                    # Adapter lifecycle management
        adapter_timeout: 1800                            # Seconds before idle adapter is unloaded
      device_group:
        name: sampler
        ranks: 1                                     # Number of GPUs to use
        device_type: cuda
      device_mesh:
        device_type: cuda
        dp_size: 1
    deployments:
      - name: SamplerManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 16
        ray_actor_options:
          num_cpus: 0.1
          runtime_env:
            env_vars:
              TWINKLE_TRUST_REMOTE_CODE: "0"
